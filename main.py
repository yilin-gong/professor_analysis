import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import concurrent.futures
from openai import OpenAI
import urllib.parse
import logging
import os
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import re
from typing import Dict, List, Tuple, Optional, Union
import json

# Set up logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


# åˆ›å»ºOpenAIå®¢æˆ·ç«¯å‡½æ•°
def get_client(api_key: str) -> OpenAI:
    """ä½¿ç”¨æä¾›çš„APIå¯†é’¥åˆ›å»ºOpenAIå®¢æˆ·ç«¯
    
    Args:
        api_key: APIå¯†é’¥å­—ç¬¦ä¸²
        
    Returns:
        OpenAIå®¢æˆ·ç«¯å®ä¾‹
        
    Raises:
        ValueError: å¦‚æœAPIå¯†é’¥ä¸ºç©º
    """
    if not api_key:
        raise ValueError("å¿…é¡»æä¾›APIå¯†é’¥")
    return OpenAI(api_key=api_key, base_url="https://ark.cn-beijing.volces.com/api/v3")


# OpenAIå®¢æˆ·ç«¯å°†åœ¨è°ƒç”¨æ—¶åˆå§‹åŒ–
client = None


# Create a session with retry capability
def create_session() -> requests.Session:
    """åˆ›å»ºå…·æœ‰é‡è¯•åŠŸèƒ½çš„HTTPä¼šè¯
    
    Returns:
        é…ç½®äº†é‡è¯•ç­–ç•¥çš„requests.Sessionå¯¹è±¡
    """
    session = requests.Session()
    retries = Retry(
        total=3, backoff_factor=0.5, status_forcelist=[429, 500, 502, 503, 504]
    )
    session.mount("http://", HTTPAdapter(max_retries=retries))
    session.mount("https://", HTTPAdapter(max_retries=retries))
    return session


def detect_next_page(soup: BeautifulSoup, base_url: str) -> Optional[str]:
    """æ£€æµ‹åˆ†é¡µä¸­çš„ä¸‹ä¸€é¡µé“¾æ¥
    
    Args:
        soup: å½“å‰é¡µé¢çš„BeautifulSoupå¯¹è±¡
        base_url: åŸºç¡€URLï¼Œç”¨äºè§£æç›¸å¯¹é“¾æ¥
        
    Returns:
        ä¸‹ä¸€é¡µçš„URLï¼Œå¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ™è¿”å›None
    """
    next_link = soup.find("a", rel=lambda x: x and "next" in x.lower())
    if next_link and next_link.get("href"):
        href = next_link["href"]
        if not href.startswith(("http://", "https://")):
            return urllib.parse.urljoin(base_url, href)
        return href

    for anchor in soup.find_all("a", href=True):
        text = anchor.get_text(strip=True).lower()
        if text in ("next", "next page", ">", "Â»", "ä¸‹ä¸€é¡µ"):
            href = anchor["href"]
            if not href.startswith(("http://", "https://")):
                return urllib.parse.urljoin(base_url, href)
            return href
    return None


def get_all_links(
    url: str, 
    session: Optional[requests.Session] = None, 
    follow_pagination: bool = False, 
    max_pages: int = 3
) -> List[str]:
    """ä»ç½‘é¡µä¸­æå–é“¾æ¥ï¼Œå¯é€‰æ‹©è·Ÿéšåˆ†é¡µ
    
    Args:
        url: èµ·å§‹ç½‘é¡µURL
        session: HTTPä¼šè¯å¯¹è±¡ï¼Œå¦‚æœä¸ºNoneåˆ™åˆ›å»ºæ–°çš„
        follow_pagination: æ˜¯å¦è·Ÿéšåˆ†é¡µé“¾æ¥
        max_pages: æœ€å¤šè·Ÿéšçš„é¡µé¢æ•°é‡
        
    Returns:
        é«˜è´¨é‡é“¾æ¥çš„åˆ—è¡¨ï¼ŒæŒ‰è¯„åˆ†æ’åº
        
    Raises:
        Exception: ç½‘ç»œè¯·æ±‚æˆ–è§£æé”™è¯¯æ—¶
    """
    if session is None:
        session = create_session()

    to_visit = [url]
    visited = set()
    collected = []
    page_count = 0

    while to_visit and page_count < max_pages:
        current_url = to_visit.pop(0)
        if current_url in visited:
            continue
        visited.add(current_url)

        try:
            logger.info(f"Extracting links from {current_url}")
            response = session.get(
                current_url,
                timeout=10,
                headers={
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
                },
            )
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "html.parser")
        except Exception as e:
            logger.error(f"Error scraping {current_url}: {e}")
            continue

        # Parse the URL for resolving relative links
        parsed_url = urllib.parse.urlparse(current_url)
        base_url = f"{parsed_url.scheme}://{parsed_url.netloc}"

        # Check for base tag
        base_tag = soup.find("base", href=True)
        if base_tag and base_tag["href"]:
            base_href = base_tag["href"]
            if base_href.startswith(("http://", "https://")):
                base_url = base_href
            else:
                base_url = urllib.parse.urljoin(base_url, base_href)

        # åˆ†æé¡µé¢ç»“æ„
        page_structure = analyze_page_structure(soup)

        links = []

        for anchor in soup.find_all("a", href=True):
            href = anchor["href"].strip()
            anchor_text = anchor.get_text(strip=True).lower()

            # åŸºç¡€è¿‡æ»¤
            if not href or href.startswith(("javascript:", "mailto:", "tel:", "#")):
                continue

            if not href.startswith(("http://", "https://")):
                href = urllib.parse.urljoin(base_url, href)

            # æ–‡ä»¶ç±»å‹è¿‡æ»¤
            if href.lower().endswith((".pdf", ".jpg", ".jpeg", ".png", ".gif", ".zip", ".doc", ".docx", ".ppt", ".pptx")):
                continue

            # ä½¿ç”¨æ–°çš„è¯„åˆ†ç³»ç»Ÿ
            score = calculate_link_score(anchor, href, page_structure, base_url)
            
            # æ£€æµ‹é¡µé¢ç±»å‹å¹¶åŠ¨æ€è°ƒæ•´é˜ˆå€¼
            is_academic_page = detect_academic_page_type(base_url, page_structure)
            threshold = 1.5 if is_academic_page else 2  # å­¦æœ¯é¡µé¢ä½¿ç”¨æ›´ä½é˜ˆå€¼
            
            # åªä¿ç•™è¯„åˆ†å¤§äºé˜ˆå€¼çš„é“¾æ¥
            if score > threshold:
                links.append((href, score))
                if score >= 5:  # é«˜åˆ†é“¾æ¥è®°å½•æ—¥å¿—
                    logger.info(f"High-score professor link found: {href} (Score: {score}) - Text: {anchor_text}")
                elif is_academic_page and score > 3:
                    logger.info(f"Academic page link found: {href} (Score: {score}) - Text: {anchor_text}")

        collected.extend(links)

        if follow_pagination:
            next_url = detect_next_page(soup, base_url)
            if next_url and next_url not in visited:
                to_visit.append(next_url)

        page_count += 1

    # Remove duplicates while prioritizing higher-scored links
    unique_links = []
    seen = set()
    # æŒ‰åˆ†æ•°ä»é«˜åˆ°ä½æ’åºï¼Œä¼˜å…ˆå¤„ç†é«˜åˆ†é“¾æ¥
    for link, score in sorted(collected, key=lambda x: x[1], reverse=True):
        if link not in seen:
            seen.add(link)
            unique_links.append(link)

    logger.info(
        f"Found {len(unique_links)} high-quality links from {url} across {page_count} page(s)"
    )
    return unique_links


def is_professor_webpage(url, session=None, client=None):
    """Use LLM to determine if a webpage is a professor's personal page and extract basic info."""
    if session is None:
        session = create_session()

    if client is None:
        raise ValueError("å¿…é¡»æä¾›APIå®¢æˆ·ç«¯")

    try:
        logger.info(f"Checking if {url} is a professor webpage")
        response = robust_web_request(session, url)
        soup = BeautifulSoup(response.text, "html.parser")

        # Extract title and meta description
        title = soup.title.string if soup.title else ""
        meta_desc = ""
        meta_tag = soup.find("meta", attrs={"name": "description"})
        if meta_tag and "content" in meta_tag.attrs:
            meta_desc = meta_tag["content"]

        # Extract text content
        for script in soup(["script", "style"]):
            script.extract()

        text = soup.get_text(separator=" ", strip=True)
        # Limit text length for API efficiency
        text = text[:3000]

        # Look for professor-specific keywords
        professor_keywords = [
            "professor",
            "faculty",
            "dr.",
            "ph.d",
            "phd",
            "research interests",
            "publications",
            "curriculum vitae",
            "cv",
            "teaching",
            "associate professor",
            "assistant professor",
            "scholar",
        ]
        keyword_matches = [
            kw
            for kw in professor_keywords
            if kw.lower() in text.lower() or kw.lower() in title.lower()
        ]

        # å¯»æ‰¾H1æ ‡ç­¾ä½œä¸ºå¯èƒ½çš„å§“åæ¥æº
        h1_text = ""
        h1_tag = soup.find("h1")
        if h1_tag:
            h1_text = h1_tag.get_text(strip=True)

        # Combine important elements for LLM analysis
        important_elements = f"URL: {url}\nTitle: {title}\nH1 Tag: {h1_text}\nMeta Description: {meta_desc}\n\nContent Preview: {text[:500]}\n\nDetected Keywords: {', '.join(keyword_matches)}"

        logger.info(f"Title: {title}")
        logger.info(f"Keyword matches: {keyword_matches}")

        # Ask LLM to classify the page and extract structured information
        response_text = robust_llm_call(client, [
            {
                "role": "system",
                "content": """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­¦æœ¯ç½‘é¡µåˆ†æä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯ï¼š
1. åˆ¤æ–­è¿™æ˜¯å¦æ˜¯å¤§å­¦æˆ–ç ”ç©¶æœºæ„çš„æ•™æˆã€æ•™å‘˜æˆ–å­¦æœ¯ç ”ç©¶äººå‘˜çš„ä¸ªäººé¡µé¢
2. å¦‚æœæ˜¯ï¼Œæå–ä»¥ä¸‹ç»“æ„åŒ–ä¿¡æ¯ï¼šå§“åã€èŒä½/å¤´è¡”ã€é™¢ç³»/éƒ¨é—¨

è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼š
{
    "is_professor": true/false,
    "confidence": 0.0-1.0,
    "name": "æ•™æˆå§“å",
    "title": "èŒä½å¤´è¡”",
    "department": "é™¢ç³»éƒ¨é—¨"
}

æ³¨æ„ï¼š
- å§“ååº”è¯¥æ˜¯å®Œæ•´çš„äººåï¼Œä¸åŒ…å«èŒä½å¤´è¡”
- èŒä½åŒ…æ‹¬ï¼šProfessor, Associate Professor, Assistant Professor, Dr., etc.
- é™¢ç³»æ˜¯æ‰€å±çš„å­¦é™¢æˆ–éƒ¨é—¨
- å¦‚æœä¸æ˜¯æ•™æˆé¡µé¢ï¼Œå°†é™¤is_professorå’Œconfidenceå¤–çš„å­—æ®µè®¾ä¸ºç©ºå­—ç¬¦ä¸²""",
            },
            {
                "role": "user",
                "content": f"è¯·åˆ†æä»¥ä¸‹ç½‘é¡µä¿¡æ¯ï¼š\n\n{important_elements}",
            },
        ])

        logger.info(f"LLM response for {url}: {response_text}")
        
        # å°è¯•è§£æJSONå“åº”
        try:
            import json
            result = json.loads(response_text)
            is_prof = result.get("is_professor", False)
            
            if is_prof:
                prof_info = {
                    "name": result.get("name", "Unknown"),
                    "title": result.get("title", ""),
                    "department": result.get("department", ""),
                    "confidence": result.get("confidence", 0.5)
                }
                return True, prof_info
            else:
                return False, {
                    "name": "",
                    "title": "",
                    "department": "",
                    "confidence": result.get("confidence", 0.0)
                }
        except json.JSONDecodeError:
            # å¦‚æœJSONè§£æå¤±è´¥ï¼Œä½¿ç”¨åŸæœ‰çš„ç®€å•åˆ¤æ–­é€»è¾‘
            is_prof = "true" in response_text.lower() or "yes" in response_text.lower()
            if is_prof:
                # å°è¯•ä»æ ‡é¢˜ä¸­æå–å§“å
                name = h1_text if h1_text else title
                return True, {
                    "name": name,
                    "title": "",
                    "department": "",
                    "confidence": 0.7
                }
            else:
                return False, {
                    "name": "",
                    "title": "",
                    "department": "",
                    "confidence": 0.3
                }
            
    except Exception as e:
        logger.error(f"Error analyzing {url}: {e}")
        return False, {
            "name": "",
            "title": "",
            "department": "",
            "confidence": 0.0
        }


def get_research_interests(url, session=None, client=None):
    """Use LLM to summarize a professor's research interests from their webpage and extract keywords."""
    if session is None:
        session = create_session()

    if client is None:
        raise ValueError("å¿…é¡»æä¾›APIå®¢æˆ·ç«¯")

    try:
        response = robust_web_request(session, url)
        soup = BeautifulSoup(response.text, "html.parser")

        # Get title
        title = soup.title.string if soup.title else ""

        # Extract text content
        for script in soup(["script", "style"]):
            script.extract()

        # Try to find research-specific sections
        research_sections = []
        research_keywords = [
            "research",
            "interests",
            "projects",
            "expertise",
            "publications",
        ]

        # Look for sections with research-related headers
        for header in soup.find_all(["h1", "h2", "h3", "h4"]):
            header_text = header.get_text(strip=True).lower()
            if any(keyword in header_text for keyword in research_keywords):
                section_content = []
                # Collect paragraphs until the next header
                for sibling in header.find_next_siblings():
                    if sibling.name in ["h1", "h2", "h3", "h4"]:
                        break
                    if sibling.name in ["p", "ul", "ol", "div"]:
                        section_content.append(sibling.get_text(strip=True))

                if section_content:
                    research_sections.append(
                        {"header": header_text, "content": " ".join(section_content)}
                    )

        # Combine all found sections
        research_content = ""
        if research_sections:
            research_content = "\n\n".join(
                f"{section['header']}: {section['content'][:500]}"
                for section in research_sections
            )
        else:
            # If no specific research sections found, use general page content
            research_content = soup.get_text(separator=" ", strip=True)[:3000]

        # æŸ¥æ‰¾ç›¸å…³é“¾æ¥å¹¶è·å–é¢å¤–ä¿¡æ¯
        related_links = find_related_professor_links(url, session)
        additional_info = integrate_professor_info(url, related_links, session, client)
        
        # æ„å»ºå®Œæ•´çš„ç ”ç©¶å†…å®¹ï¼ŒåŒ…å«é¢å¤–ä¿¡æ¯
        full_research_content = research_content
        if additional_info.get('additional_research_info'):
            full_research_content += f"\n\né¢å¤–ç ”ç©¶ä¿¡æ¯: {additional_info['additional_research_info'][:1000]}"
        if additional_info.get('publications_info'):
            full_research_content += f"\n\nå‡ºç‰ˆç‰©ä¿¡æ¯: {additional_info['publications_info'][:1000]}"

        # Ask LLM to extract research interests and keywords
        response_text = robust_llm_call(client, [
            {
                "role": "system",
                "content": """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­¦æœ¯ä¿¡æ¯æå–ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯ä»æ•™æˆçš„ç½‘é¡µå†…å®¹ä¸­æå–ç ”ç©¶å…´è¶£å’Œå…³é”®è¯ã€‚

è¯·è¿”å›JSONæ ¼å¼çš„ç»“æœï¼š
{
    "research_interests": "æ¸…æ´çš„ç ”ç©¶å…´è¶£æè¿°",
    "keywords": ["å…³é”®è¯1", "å…³é”®è¯2", "å…³é”®è¯3"]
}

è¦æ±‚ï¼š
1. ç ”ç©¶å…´è¶£åº”è¯¥åªåŒ…å«å…·ä½“çš„ç ”ç©¶å†…å®¹ï¼Œä¸è¦åŒ…å«"è¯¥æ•™æˆçš„ç ”ç©¶å…´è¶£æ˜¯"ç­‰æè¿°æ€§è¯­è¨€
2. ä¸è¦ä½¿ç”¨ç²—ä½“ã€æ–œä½“ç­‰æ ¼å¼åŒ–æ ‡è®°
3. å…³é”®è¯åº”è¯¥æ˜¯3-10ä¸ªæœ€é‡è¦çš„ç ”ç©¶é¢†åŸŸæœ¯è¯­
4. å…³é”®è¯åº”è¯¥æŒ‰é‡è¦æ€§æ’åº""",
            },
            {
                "role": "user",
                "content": f"è¯·åˆ†æä»¥ä¸‹æ•™æˆç½‘é¡µå†…å®¹ï¼Œæå–ç ”ç©¶å…´è¶£å’Œå…³é”®è¯ï¼š\n\né¡µé¢æ ‡é¢˜: {title}\n\nå†…å®¹:\n{full_research_content}",
            },
        ])

        # å°è¯•è§£æJSONå“åº”
        try:
            import json
            result = json.loads(response_text)
            
            # è·å–ç ”ç©¶å…´è¶£å¹¶è¿›è¡Œé¢å¤–æ¸…ç†
            raw_interests = result.get("research_interests", "")
            clean_interests = clean_research_text(raw_interests)
            
            # è·å–å…³é”®è¯
            keywords = result.get("keywords", [])
            
            # ç¡®ä¿å…³é”®è¯æ˜¯åˆ—è¡¨ä¸”ä¸è¶…è¿‡10ä¸ª
            if isinstance(keywords, list):
                keywords = keywords[:10]
            else:
                keywords = []
            
            return {
                "interests": clean_interests,
                "keywords": keywords,
                "additional_urls": additional_info.get('additional_urls', []),
            }
            
        except json.JSONDecodeError:
            # å¦‚æœJSONè§£æå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ–‡æœ¬å¹¶å°è¯•ç®€å•æ¸…ç†
            clean_interests = clean_research_text(response_text)
            return {
                "interests": clean_interests,
                "keywords": [],
                "additional_urls": additional_info.get('additional_urls', []),
            }
            
    except Exception as e:
        logger.error(f"Error extracting research interests from {url}: {e}")
        return {
            "interests": "Unable to determine research interests",
            "keywords": [],
            "additional_urls": [],
        }


def process_link(link, session, client=None):
    """Process a single link - to be used with ThreadPoolExecutor."""
    logger.info(f"Analyzing: {link}")

    if client is None:
        raise ValueError("å¿…é¡»æä¾›APIå®¢æˆ·ç«¯")

    try:
        is_professor, prof_info = is_professor_webpage(link, session, client)
        if is_professor:
            research_data = get_research_interests(link, session, client)
            return {
                "URL": link,
                "Professor Name": prof_info.get("name", "Unknown"),
                "Title": prof_info.get("title", ""),
                "Department": prof_info.get("department", ""),
                "Is Professor Page": "Yes",
                "Research Interests": research_data.get("interests", ""),
                "Keywords": research_data.get("keywords", []),
                "Additional URLs": research_data.get("additional_urls", []),
                "Confidence Score": prof_info.get("confidence", 0.0)
            }
        else:
            return {
                "URL": link,
                "Professor Name": "",
                "Title": "",
                "Department": "",
                "Is Professor Page": "No",
                "Research Interests": "",
                "Keywords": [],
                "Additional URLs": [],
                "Confidence Score": 0.0
            }
    except Exception as e:
        logger.error(f"Error processing {link}: {e}")
        return {
            "URL": link,
            "Professor Name": "",
            "Title": "",
            "Department": "",
            "Is Professor Page": "Error",
            "Research Interests": f"Error: {str(e)}",
            "Keywords": [],
            "Additional URLs": [],
            "Confidence Score": 0.0
        }


def analyze_webpage_links(start_url, api_key, max_links=50, max_workers=5, max_pages=3):
    """Analyze a website to find professor pages and research interests.

    Parameters
    ----------
    start_url : str
        The starting page URL for scraping.
    api_key : str
        API key used to access the LLM service.
    max_links : int, optional
        Maximum number of links to analyze.
    max_workers : int, optional
        Number of threads used for processing.
    max_pages : int, optional
        Maximum pages to follow when detecting pagination.
    """
    logger.info(f"Starting analysis from: {start_url}")

    # ä½¿ç”¨æä¾›çš„APIå¯†é’¥åˆå§‹åŒ–å®¢æˆ·ç«¯
    if not api_key:
        raise ValueError("å¿…é¡»æä¾›APIå¯†é’¥æ‰èƒ½æ‰§è¡Œåˆ†æ")

    client = get_client(api_key)

    # Create a session for consistent connections
    session = create_session()

    # Get all links from the starting URL, following pagination if present
    all_links = get_all_links(
        start_url, session, follow_pagination=True, max_pages=max_pages
    )[:max_links]
    logger.info(f"Found {len(all_links)} links to analyze.")

    if not all_links:
        logger.warning(
            "No links found to analyze. Check if the website is accessible or has changed structure."
        )
        return pd.DataFrame(columns=["URL", "Professor Name", "Title", "Department", "Is Professor Page", "Research Interests", "Keywords", "Additional URLs", "Confidence Score"])

    # Print some of the links we're going to check
    for i, link in enumerate(all_links[:10]):
        logger.info(f"Link {i+1} to check: {link}")

    results = []

    # Use ThreadPoolExecutor for parallel processing
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_link = {
            executor.submit(process_link, link, session, client): link
            for link in all_links
        }
        for future in concurrent.futures.as_completed(future_to_link):
            link = future_to_link[future]
            try:
                result = future.result()
                if result:
                    results.append(result)
                    # Log professor pages as we find them
                    if result["Is Professor Page"] == "Yes":
                        logger.info(f"FOUND PROFESSOR PAGE: {link}")
                        logger.info(
                            f"Research Interests: {result['Research Interests']}"
                        )
            except Exception as e:
                logger.error(f"Exception processing {link}: {e}")
                results.append(
                    {
                        "URL": link,
                        "Is Professor Page": "Error",
                        "Research Interests": f"Error: {str(e)}",
                    }
                )

    # Create DataFrame
    df = pd.DataFrame(results)

    # Filter only professor pages if desired
    professors_df = df[df["Is Professor Page"] == "Yes"]

    logger.info(
        f"Analysis complete. Found {len(professors_df)} professor pages out of {len(results)} total links analyzed."
    )

    # Print summary statistics
    yes_count = sum(1 for r in results if r["Is Professor Page"] == "Yes")
    no_count = sum(1 for r in results if r["Is Professor Page"] == "No")
    error_count = sum(1 for r in results if r["Is Professor Page"] == "Error")

    logger.info(
        f"Results Summary: {yes_count} professor pages, {no_count} non-professor pages, {error_count} errors"
    )

    return df


def analyze_page_structure(soup):
    """åˆ†æé¡µé¢ç»“æ„ï¼Œè¯†åˆ«å¯¼èˆªã€å†…å®¹ã€é¡µè„šç­‰åŒºåŸŸ"""
    structure = {
        'navigation_elements': [],
        'content_elements': [],
        'footer_elements': [],
        'sidebar_elements': []
    }
    
    # è¯†åˆ«å¯¼èˆªåŒºåŸŸ
    nav_selectors = ['nav', 'header', '.navigation', '.nav', '.menu', '.navbar', '#nav', '#navigation', '#menu']
    for selector in nav_selectors:
        elements = soup.select(selector)
        structure['navigation_elements'].extend(elements)
    
    # è¯†åˆ«é¡µè„šåŒºåŸŸ
    footer_selectors = ['footer', '.footer', '#footer', '.site-footer']
    for selector in footer_selectors:
        elements = soup.select(selector)
        structure['footer_elements'].extend(elements)
    
    # è¯†åˆ«ä¾§è¾¹æ åŒºåŸŸ
    sidebar_selectors = ['aside', '.sidebar', '.side-nav', '#sidebar', '.widget-area']
    for selector in sidebar_selectors:
        elements = soup.select(selector)
        structure['sidebar_elements'].extend(elements)
    
    # è¯†åˆ«ä¸»è¦å†…å®¹åŒºåŸŸ
    content_selectors = ['main', '.content', '.main-content', '#content', '#main', 'article', '.post-content']
    for selector in content_selectors:
        elements = soup.select(selector)
        structure['content_elements'].extend(elements)
    
    return structure


def is_link_in_non_content_area(anchor, page_structure):
    """åˆ¤æ–­é“¾æ¥æ˜¯å¦åœ¨éå†…å®¹åŒºåŸŸï¼ˆå¯¼èˆªã€é¡µè„šã€ä¾§è¾¹æ ï¼‰"""
    for area_name in ['navigation_elements', 'footer_elements', 'sidebar_elements']:
        for element in page_structure[area_name]:
            if element and anchor in element.find_all('a'):
                return True
    return False


def calculate_link_score(anchor, href, page_structure, base_url):
    """è®¡ç®—é“¾æ¥çš„ç»¼åˆè¯„åˆ† (0-10åˆ†åˆ¶)"""
    score = 0
    anchor_text = anchor.get_text(strip=True).lower()
    original_anchor_text = anchor.get_text(strip=True)  # ä¿ç•™åŸå§‹å¤§å°å†™
    
    # åŸºç¡€åˆ†æ•°
    score += 1
    
    # æ£€æµ‹é¡µé¢ç±»å‹ - å­¦æœ¯æœºæ„ç‰¹æ®Šå¤„ç†
    is_academic_page = detect_academic_page_type(base_url, page_structure)
    
    # æ•™æˆç›¸å…³å…³é”®è¯åŒ¹é… (æœ€é«˜4åˆ†)
    professor_keywords = [
        "faculty", "professor", "staff", "people", "members", 
        "researchers", "team", "dr.", "ph.d", "phd", "associate professor",
        "assistant professor", "clinical professor", "emeritus"
    ]
    strong_keywords = ["professor", "faculty", "dr.", "ph.d"]
    
    keyword_matches = sum(1 for kw in professor_keywords if kw in anchor_text)
    strong_matches = sum(1 for kw in strong_keywords if kw in anchor_text)
    
    score += min(keyword_matches * 0.5 + strong_matches, 4)
    
    # URLè·¯å¾„åˆ†æ (æœ€é«˜3åˆ†)
    url_keywords = ["faculty", "professor", "people", "staff", "members", "user"]
    if any(kw in href.lower() for kw in url_keywords):
        score += 2
        
    # ç‰¹æ®Šå¤„ç†ï¼šä¸ªäººé¡µé¢æ¨¡å¼ (å¦‚ /user/æ•°å­—, /people/name)
    if re.search(r'/user/\d+$', href):
        score += 3  # è¿™å¾ˆå¯èƒ½æ˜¯ä¸ªäººé¡µé¢
    elif re.search(r'/people/[a-z\-]+$', href.lower()):
        score += 3  # NYU Steinhardtç±»å‹çš„ä¸ªäººé¡µé¢
        
    # å¢å¼ºçš„äººåè¯†åˆ« (æ”¯æŒæ›´å¤šæ ¼å¼)
    name_score = calculate_name_likelihood(original_anchor_text, is_academic_page)
    score += name_score
    
    # å­¦æœ¯é¡µé¢ç‰¹æ®ŠåŠ åˆ†
    if is_academic_page:
        # åœ¨å­¦æœ¯é¡µé¢ä¸­ï¼Œç®€å•çš„äººåé“¾æ¥åº”è¯¥å¾—åˆ°æ›´é«˜åˆ†æ•°
        if len(anchor_text.split()) == 2 and not any(kw in anchor_text for kw in professor_keywords):
            # å¯èƒ½æ˜¯çº¯äººåé“¾æ¥
            score += 2
            logger.debug(f"Academic page name bonus: {original_anchor_text} -> +2")
    
    # é¡µé¢ä½ç½®åˆ†æ (æ‰£åˆ†æœºåˆ¶) - é™ä½æ‰£åˆ†åŠ›åº¦
    if is_link_in_non_content_area(anchor, page_structure):
        score -= 1  # å‡å°‘æ‰£åˆ†ï¼Œä»3åˆ†é™åˆ°1åˆ†
    
    # é»‘åå•å…³é”®è¯ (æ‰£åˆ†)
    blacklist_keywords = [
        'home', 'about us', 'contact us', 'news', 'events', 'login', 'search',
        'admin', 'privacy', 'terms', 'cookie', 'sitemap', 'rss', 'subscribe',
        'programs', 'admissions', 'tuition', 'apply'  # æ·»åŠ ä¸€äº›å­¦æœ¯ç½‘ç«™å¸¸è§çš„éæ•™æˆé“¾æ¥
    ]
    if any(kw in anchor_text for kw in blacklist_keywords):
        score -= 2
    
    # URLæ¨¡å¼è¿‡æ»¤ (æ‰£åˆ†)
    bad_patterns = ['/admin/', '/login/', '/search/', '/api/', '/static/', '/css/', '/js/']
    if any(pattern in href.lower() for pattern in bad_patterns):
        score -= 3
    
    # è°ƒè¯•æ—¥å¿—
    if score > 3:  # åªè®°å½•æœ‰æ½œåŠ›çš„é“¾æ¥
        logger.debug(f"Link scoring: '{original_anchor_text}' -> {href} = {score} points (academic: {is_academic_page})")
    
    # ç¡®ä¿åˆ†æ•°åœ¨0-10èŒƒå›´å†…
    return max(0, min(10, score))


def detect_academic_page_type(base_url: str, page_structure: Dict) -> bool:
    """æ£€æµ‹æ˜¯å¦ä¸ºå­¦æœ¯æœºæ„é¡µé¢"""
    # URLæ¨¡å¼æ£€æµ‹
    academic_url_patterns = [
        'steinhardt.nyu.edu', 'faculty', 'people', 'staff', 'edu/',
        'university', 'college', 'school', 'department'
    ]
    
    if any(pattern in base_url.lower() for pattern in academic_url_patterns):
        return True
    
    # é¡µé¢ç»“æ„æ£€æµ‹ - æŸ¥æ‰¾å­¦æœ¯ç›¸å…³å…ƒç´ 
    if page_structure:
        # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤šé¡µé¢ç»“æ„æ£€æµ‹é€»è¾‘
        pass
    
    return False


def calculate_name_likelihood(text: str, is_academic_page: bool) -> int:
    """è®¡ç®—æ–‡æœ¬æ˜¯äººåçš„å¯èƒ½æ€§è¯„åˆ† (0-4åˆ†)"""
    if not text or len(text) > 50:  # è¿‡é•¿çš„æ–‡æœ¬ä¸å¤ªå¯èƒ½æ˜¯äººå
        return 0
    
    score = 0
    words = text.split()
    
    # åŸºæœ¬äººåæ¨¡å¼
    if len(words) == 2:
        # ä¸¤ä¸ªè¯çš„ç»„åˆï¼Œå¾ˆå¯èƒ½æ˜¯å§“å
        first, last = words
        if (first[0].isupper() and last[0].isupper() and 
            len(first) > 1 and len(last) > 1):
            score += 3
            
    elif len(words) == 3:
        # ä¸‰ä¸ªè¯çš„ç»„åˆï¼Œå¯èƒ½æ˜¯ "First Middle Last" æˆ– "Dr. First Last"
        if all(word[0].isupper() for word in words if len(word) > 1):
            score += 2
            
    # å­¦æœ¯é¡µé¢ä¸Šçš„ç®€åŒ–åç§°æ ¼å¼
    if is_academic_page and len(words) >= 2:
        # åœ¨å­¦æœ¯é¡µé¢ä¸Šï¼Œå³ä½¿æ ¼å¼ä¸æ ‡å‡†ä¹Ÿç»™äºˆä¸€å®šåˆ†æ•°
        if all(word[0].isupper() for word in words if len(word) > 1):
            score += 1
    
    # å¸¸è§å­¦æœ¯ç§°è°“æ£€æµ‹
    academic_titles = ['dr.', 'prof.', 'professor', 'dr', 'prof']
    if any(title in text.lower() for title in academic_titles):
        score += 1
    
    # ç‰¹æ®Šå­—ç¬¦æ£€æµ‹ï¼ˆäººåä¸­ä¸å¤ªå¯èƒ½å‡ºç°çš„å­—ç¬¦ï¼‰
    if any(char in text for char in ['@', '#', '$', '%', '&', '*', '(', ')', '[', ']']):
        score -= 2
    
    return max(0, min(4, score))


def clean_research_text(raw_text):
    """æ¸…ç†ç ”ç©¶å…´è¶£æ–‡æœ¬ï¼Œå»é™¤æ ¼å¼åŒ–æ ‡è®°å’Œæè¿°æ€§è¯­è¨€"""
    if not raw_text:
        return ""
    
    # å»é™¤markdownæ ¼å¼åŒ–
    text = re.sub(r'\*\*(.*?)\*\*', r'\1', raw_text)  # å»é™¤ç²—ä½“
    text = re.sub(r'\*(.*?)\*', r'\1', text)  # å»é™¤æ–œä½“
    text = re.sub(r'`(.*?)`', r'\1', text)  # å»é™¤ä»£ç æ ‡è®°
    text = re.sub(r'#{1,6}\s*', '', text)  # å»é™¤æ ‡é¢˜æ ‡è®°
    
    # å»é™¤HTMLæ ‡ç­¾æ®‹ç•™
    text = re.sub(r'<[^>]+>', '', text)
    
    # å»é™¤æè¿°æ€§å¼€å¤´è¯­å¥
    descriptive_patterns = [
        r'^.*?æ•™æˆ.*?ç ”ç©¶å…´è¶£.*?[:ï¼š]',
        r'^.*?professor.*?research interests.*?[:ï¼š]',
        r'^.*?ä¸»è¦ç ”ç©¶.*?[:ï¼š]',
        r'^.*?ç ”ç©¶é¢†åŸŸ.*?[:ï¼š]',
        r'^.*?focuses on.*?[:ï¼š]',
        r'^.*?specializes in.*?[:ï¼š]',
        r'^.*?çš„ç ”ç©¶å…´è¶£åŒ…æ‹¬.*?[:ï¼š]',
        r'^.*?research area.*?[:ï¼š]',
    ]
    
    for pattern in descriptive_patterns:
        text = re.sub(pattern, '', text, flags=re.IGNORECASE)
    
    # å»é™¤ç»“å°¾çš„æè¿°æ€§è¯­å¥
    ending_patterns = [
        r'ç­‰ç›¸å…³é¢†åŸŸ.*$',
        r'ç­‰ç ”ç©¶æ–¹å‘.*$',
        r'and related areas.*$',
        r'among others.*$',
    ]
    
    for pattern in ending_patterns:
        text = re.sub(pattern, '', text, flags=re.IGNORECASE)
    
    # æ¸…ç†å¤šä½™çš„ç©ºç™½å­—ç¬¦
    text = re.sub(r'\s+', ' ', text)
    text = text.strip()
    
    # å»é™¤å¼€å¤´çš„å†’å·æˆ–å¥å·
    text = re.sub(r'^[ï¼š:\.]+\s*', '', text)
    
    return text


def extract_keywords(research_text):
    """ä»ç ”ç©¶å…´è¶£æ–‡æœ¬ä¸­æå–3-10ä¸ªå…³é”®è¯"""
    if not research_text:
        return []
    
    # è¿™ä¸ªå‡½æ•°å°†ç”±LLMæ¥å®ç°ï¼Œè¿”å›å…³é”®è¯åˆ—è¡¨
    return []  # å ä½ç¬¦ï¼Œå°†åœ¨get_research_interestsä¸­é€šè¿‡LLMå®ç°


def find_related_professor_links(url, session=None):
    """åœ¨æ•™æˆé¡µé¢å†…æŸ¥æ‰¾ç›¸å…³çš„CVã€å‡ºç‰ˆç‰©ã€ç ”ç©¶ç­‰é“¾æ¥"""
    if session is None:
        session = create_session()
    
    related_links = []
    
    try:
        response = session.get(url, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")
        
        # è§£æURLä»¥ä¾¿å¤„ç†ç›¸å¯¹é“¾æ¥
        parsed_url = urllib.parse.urlparse(url)
        base_url = f"{parsed_url.scheme}://{parsed_url.netloc}"
        
        # ç›¸å…³é“¾æ¥çš„å…³é”®è¯æ¨¡å¼
        related_keywords = {
            'cv': ['cv', 'curriculum vitae', 'resume', 'ç®€å†'],
            'publications': ['publications', 'papers', 'articles', 'published', 'å‘è¡¨', 'è®ºæ–‡', 'å‡ºç‰ˆç‰©'],
            'research': ['research', 'projects', 'ongoing', 'current', 'ç ”ç©¶', 'é¡¹ç›®'],
            'teaching': ['teaching', 'courses', 'syllabi', 'æ•™å­¦', 'è¯¾ç¨‹'],
            'bio': ['biography', 'bio', 'about', 'ç®€ä»‹', 'ä¸ªäººä»‹ç»']
        }
        
        for anchor in soup.find_all("a", href=True):
            href = anchor["href"].strip()
            anchor_text = anchor.get_text(strip=True).lower()
            
            # è·³è¿‡æ— æ•ˆé“¾æ¥
            if not href or href.startswith(("javascript:", "mailto:", "tel:", "#")):
                continue
                
            # å¤„ç†ç›¸å¯¹é“¾æ¥
            if not href.startswith(("http://", "https://")):
                href = urllib.parse.urljoin(base_url, href)
            
            # è·³è¿‡å¤–éƒ¨é“¾æ¥ï¼ˆä¸åœ¨åŒä¸€åŸŸåä¸‹ï¼‰
            if not href.startswith(base_url):
                continue
                
            # æ£€æŸ¥æ˜¯å¦åŒ¹é…ç›¸å…³å…³é”®è¯
            for category, keywords in related_keywords.items():
                if any(keyword in anchor_text for keyword in keywords) or \
                   any(keyword in href.lower() for keyword in keywords):
                    related_links.append({
                        'url': href,
                        'text': anchor_text,
                        'category': category
                    })
                    break
        
        # å»é‡
        seen_urls = set()
        unique_links = []
        for link in related_links:
            if link['url'] not in seen_urls:
                seen_urls.add(link['url'])
                unique_links.append(link)
        
        logger.info(f"Found {len(unique_links)} related links on {url}")
        return unique_links
        
    except Exception as e:
        logger.error(f"Error finding related links on {url}: {e}")
        return []


def integrate_professor_info(main_url, related_links, session=None, client=None):
    """æ•´åˆæ•™æˆä¸»é¡µé¢å’Œç›¸å…³é¡µé¢çš„ä¿¡æ¯"""
    if not client:
        return {}
    
    integrated_info = {
        'additional_research_info': '',
        'cv_info': '',
        'publications_info': '',
        'additional_urls': []
    }
    
    # è®°å½•æ‰€æœ‰ç›¸å…³URL
    integrated_info['additional_urls'] = [link['url'] for link in related_links]
    
    # å°è¯•è·å–CVå’Œå‡ºç‰ˆç‰©é¡µé¢çš„é¢å¤–ä¿¡æ¯
    high_value_categories = ['cv', 'publications', 'research']
    
    for link in related_links:
        if link['category'] in high_value_categories:
            try:
                if session is None:
                    session = create_session()
                    
                response = session.get(link['url'], timeout=10)
                response.raise_for_status()
                soup = BeautifulSoup(response.text, "html.parser")
                
                # æå–é¡µé¢å†…å®¹
                for script in soup(["script", "style"]):
                    script.extract()
                
                content = soup.get_text(separator=" ", strip=True)[:2000]  # é™åˆ¶é•¿åº¦
                
                # æ ¹æ®ç±»åˆ«å­˜å‚¨ä¿¡æ¯
                if link['category'] == 'cv':
                    integrated_info['cv_info'] += f" {content}"
                elif link['category'] == 'publications':
                    integrated_info['publications_info'] += f" {content}"
                elif link['category'] == 'research':
                    integrated_info['additional_research_info'] += f" {content}"
                    
            except Exception as e:
                logger.warning(f"Could not fetch additional info from {link['url']}: {e}")
                continue
    
    return integrated_info


def robust_llm_call(client, messages, max_retries=3, backoff_factor=1.0):
    """ç¨³å¥çš„LLMè°ƒç”¨ï¼Œå¸¦æœ‰é‡è¯•æœºåˆ¶"""
    import time
    
    for attempt in range(max_retries):
        try:
            response = client.chat.completions.create(
                model="doubao-1-5-pro-32k-250115",
                messages=messages,
                timeout=30  # å¢åŠ è¶…æ—¶æ—¶é—´
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            logger.warning(f"LLM call attempt {attempt + 1} failed: {e}")
            if attempt < max_retries - 1:
                # æŒ‡æ•°é€€é¿
                sleep_time = backoff_factor * (2 ** attempt)
                time.sleep(sleep_time)
            else:
                logger.error(f"LLM call failed after {max_retries} attempts")
                raise e


def robust_web_request(session, url, max_retries=3, timeout=15):
    """ç¨³å¥çš„ç½‘ç»œè¯·æ±‚ï¼Œå¸¦æœ‰é‡è¯•æœºåˆ¶"""
    import time
    
    for attempt in range(max_retries):
        try:
            response = session.get(
                url,
                timeout=timeout,
                headers={
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
                },
            )
            response.raise_for_status()
            return response
        except Exception as e:
            logger.warning(f"Web request attempt {attempt + 1} for {url} failed: {e}")
            if attempt < max_retries - 1:
                time.sleep(1.0 * (attempt + 1))  # çº¿æ€§é€€é¿
            else:
                logger.error(f"Web request to {url} failed after {max_retries} attempts")
                raise e


def intelligent_parameter_estimation(url: str, session: Optional[requests.Session] = None) -> Dict[str, int]:
    """æ™ºèƒ½ä¼°è®¡æœ€ä¼˜çš„åˆ†æå‚æ•°
    
    Args:
        url: èµ·å§‹é¡µé¢URL
        session: HTTPä¼šè¯å¯¹è±¡
        
    Returns:
        åŒ…å«æ¨èå‚æ•°çš„å­—å…¸ {'max_links': int, 'max_pages': int, 'reasoning': str}
    """
    if session is None:
        session = create_session()
    
    try:
        # è·å–é¦–é¡µå†…å®¹
        response = robust_web_request(session, url)
        soup = BeautifulSoup(response.text, "html.parser")
        
        # 1. åˆ†æé¡µé¢è§„æ¨¡å’Œç±»å‹
        page_analysis = analyze_page_characteristics(soup, url)
        
        # 2. æ£€æµ‹åˆ†é¡µç»“æ„
        pagination_info = analyze_pagination_structure(soup, url)
        
        # 3. è®¡ç®—æ•™æˆé“¾æ¥å¯†åº¦
        professor_density = calculate_professor_link_density(soup, url)
        
        # 4. åŸºäºåˆ†æç»“æœæ¨èå‚æ•°
        recommendations = generate_parameter_recommendations(
            page_analysis, pagination_info, professor_density
        )
        
        logger.info(f"æ™ºèƒ½å‚æ•°æ¨è: {recommendations}")
        return recommendations
        
    except Exception as e:
        logger.warning(f"æ™ºèƒ½å‚æ•°ä¼°è®¡å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼: {e}")
        return {
            'max_links': 30,
            'max_pages': 3,
            'reasoning': 'å‚æ•°ä¼°è®¡å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼'
        }


def analyze_page_characteristics(soup: BeautifulSoup, url: str) -> Dict[str, any]:
    """åˆ†æé¡µé¢ç‰¹å¾å’Œè§„æ¨¡"""
    analysis = {
        'page_type': 'unknown',
        'estimated_scale': 'medium',
        'has_search_filters': False,
        'total_links': 0,
        'content_depth': 'shallow'
    }
    
    # åˆ†æURLè·¯å¾„ï¼Œåˆ¤æ–­é¡µé¢ç±»å‹
    if any(keyword in url.lower() for keyword in ['/faculty', '/people', '/staff']):
        if 'department' in url.lower() or len(url.split('/')) > 5:
            analysis['page_type'] = 'department'  # ç³»çº§é¡µé¢
        elif 'college' in url.lower() or 'school' in url.lower():
            analysis['page_type'] = 'college'     # å­¦é™¢çº§é¡µé¢
        else:
            analysis['page_type'] = 'faculty_list'  # é€šç”¨æ•™æˆåˆ—è¡¨
    
    # ç‰¹æ®Šæ£€æµ‹NYU Steinhardtç±»å‹é¡µé¢
    if 'steinhardt.nyu.edu' in url.lower() and 'faculty' in url.lower():
        analysis['page_type'] = 'nyu_steinhardt'
        analysis['estimated_scale'] = 'large'
    
    # æ£€æµ‹æœç´¢å’Œè¿‡æ»¤åŠŸèƒ½
    filter_indicators = soup.find_all(['select', 'input', 'form'])
    analysis['has_search_filters'] = len(filter_indicators) > 2
    
    # ç»Ÿè®¡é“¾æ¥æ€»æ•°
    all_links = soup.find_all('a', href=True)
    analysis['total_links'] = len(all_links)
    
    # è¯„ä¼°å†…å®¹æ·±åº¦
    text_content = soup.get_text()
    if len(text_content) > 10000:
        analysis['content_depth'] = 'deep'
    elif len(text_content) > 3000:
        analysis['content_depth'] = 'medium'
    else:
        analysis['content_depth'] = 'shallow'
    
    return analysis


def analyze_pagination_structure(soup: BeautifulSoup, url: str) -> Dict[str, any]:
    """åˆ†æåˆ†é¡µç»“æ„"""
    import re  # å°†å¯¼å…¥ç§»åˆ°å‡½æ•°å¼€å¤´
    
    pagination_info = {
        'has_pagination': False,
        'pagination_type': 'none',
        'estimated_total_pages': 1,
        'items_per_page': 0
    }
    
    # æŸ¥æ‰¾åˆ†é¡µæŒ‡ç¤ºå™¨
    pagination_selectors = [
        '.pagination', '.pager', '.page-numbers', 
        '[class*="page"]', '[class*="pagination"]'
    ]
    
    pagination_elements = []
    for selector in pagination_selectors:
        elements = soup.select(selector)
        pagination_elements.extend(elements)
    
    if pagination_elements:
        pagination_info['has_pagination'] = True
        
        # å°è¯•æ‰¾åˆ°é¡µç æ•°å­—
        page_numbers = []
        for element in pagination_elements:
            # æŸ¥æ‰¾æ•°å­—
            numbers = re.findall(r'\b\d+\b', element.get_text())
            page_numbers.extend([int(n) for n in numbers if int(n) > 1])
        
        if page_numbers:
            pagination_info['estimated_total_pages'] = max(page_numbers)
            pagination_info['pagination_type'] = 'numbered'
        else:
            # æ£€æŸ¥æ˜¯å¦æœ‰"ä¸‹ä¸€é¡µ"ç±»å‹çš„åˆ†é¡µ
            next_indicators = ['next', 'more', '>', 'Â»', 'ä¸‹ä¸€é¡µ']
            has_next = any(indicator in soup.get_text().lower() for indicator in next_indicators)
            if has_next:
                pagination_info['estimated_total_pages'] = 5  # ä¿å®ˆä¼°è®¡
                pagination_info['pagination_type'] = 'next_only'
    
    # ä¼°è®¡æ¯é¡µæ¡ç›®æ•°ï¼ˆåŸºäºå½“å‰é¡µé¢çš„æ•™æˆæ•°é‡ï¼‰
    professor_like_elements = soup.find_all(['div', 'li', 'tr'], 
                                          string=re.compile(r'professor|dr\.|ph\.d', re.I))
    pagination_info['items_per_page'] = len(professor_like_elements)
    
    return pagination_info


def calculate_professor_link_density(soup: BeautifulSoup, url: str) -> float:
    """è®¡ç®—æ•™æˆé“¾æ¥å¯†åº¦"""
    total_links = len(soup.find_all('a', href=True))
    if total_links == 0:
        return 0.0
    
    # ä½¿ç”¨æˆ‘ä»¬çš„è¯„åˆ†ç³»ç»Ÿå¿«é€Ÿè¯„ä¼°æ•™æˆç›¸å…³é“¾æ¥
    page_structure = analyze_page_structure(soup)
    professor_links = 0
    is_academic_page = detect_academic_page_type(url, page_structure)
    
    for anchor in soup.find_all('a', href=True):
        href = anchor['href'].strip()
        if not href or href.startswith(('javascript:', 'mailto:', 'tel:', '#')):
            continue
            
        score = calculate_link_score(anchor, href, page_structure, url)
        # å¯¹å­¦æœ¯é¡µé¢ä½¿ç”¨æ›´ä½çš„é˜ˆå€¼æ¥åˆ¤æ–­æ•™æˆé“¾æ¥
        threshold = 3 if is_academic_page else 4
        if score > threshold:
            professor_links += 1
    
    density = professor_links / total_links
    logger.info(f"æ•™æˆé“¾æ¥å¯†åº¦: {professor_links}/{total_links} = {density:.2%}")
    return density


def generate_parameter_recommendations(
    page_analysis: Dict, 
    pagination_info: Dict, 
    professor_density: float
) -> Dict[str, any]:
    """åŸºäºåˆ†æç»“æœç”Ÿæˆå‚æ•°æ¨è"""
    
    # åŸºç¡€å‚æ•°
    max_links = 30
    max_pages = 3
    reasoning_parts = []
    
    # 1. æ ¹æ®é¡µé¢ç±»å‹è°ƒæ•´
    if page_analysis['page_type'] == 'department':
        max_links = 20  # ç³»çº§é¡µé¢é€šå¸¸æ•™æˆè¾ƒå°‘
        reasoning_parts.append("ç³»çº§é¡µé¢ï¼Œå‡å°‘é“¾æ¥æ•°")
    elif page_analysis['page_type'] == 'college':
        max_links = 50  # å­¦é™¢çº§é¡µé¢æ•™æˆè¾ƒå¤š
        reasoning_parts.append("å­¦é™¢çº§é¡µé¢ï¼Œå¢åŠ é“¾æ¥æ•°")
    elif page_analysis['page_type'] == 'nyu_steinhardt':
        max_links = 60  # NYU Steinhardté¡µé¢æ•™æˆå¾ˆå¤š
        reasoning_parts.append("NYU Steinhardté¡µé¢ï¼Œå¢åŠ é“¾æ¥æ•°ä»¥ç¡®ä¿è¦†ç›–æ‰€æœ‰æ•™æˆ")
    
    # 2. æ ¹æ®æ•™æˆå¯†åº¦è°ƒæ•´
    if professor_density > 0.5:  # æé«˜å¯†åº¦ï¼Œè¯´æ˜è¿™æ˜¯ä¸“é—¨çš„æ•™æˆé¡µé¢
        max_links = max(max_links, 60)  # å¢åŠ é“¾æ¥æ•°ï¼Œå› ä¸ºå‡ ä¹éƒ½æ˜¯æ•™æˆ
        reasoning_parts.append(f"æé«˜æ•™æˆå¯†åº¦({professor_density:.1%})ï¼Œå¢åŠ é“¾æ¥æ•°ä»¥è¦†ç›–æ‰€æœ‰æ•™æˆ")
    elif professor_density > 0.3:  # é«˜å¯†åº¦
        max_links = max(max_links, 40)  # é€‚åº¦å¢åŠ é“¾æ¥æ•°
        reasoning_parts.append(f"é«˜æ•™æˆå¯†åº¦({professor_density:.1%})ï¼Œå¢åŠ é“¾æ¥æ•°")
    elif professor_density < 0.1:  # ä½å¯†åº¦
        max_links = max(max_links, 50)  # å¢åŠ é“¾æ¥æ•°ï¼Œå¯»æ‰¾æ›´å¤šæ•™æˆ
        reasoning_parts.append(f"ä½æ•™æˆå¯†åº¦({professor_density:.1%})ï¼Œå¢åŠ é“¾æ¥æ•°")
    
    # 3. æ ¹æ®åˆ†é¡µæƒ…å†µè°ƒæ•´
    if pagination_info['has_pagination']:
        estimated_pages = pagination_info['estimated_total_pages']
        if estimated_pages <= 3:
            max_pages = estimated_pages
            reasoning_parts.append(f"æ£€æµ‹åˆ°{estimated_pages}é¡µï¼Œè·Ÿéšæ‰€æœ‰é¡µé¢")
        elif estimated_pages <= 10:
            max_pages = min(5, estimated_pages)
            reasoning_parts.append(f"æ£€æµ‹åˆ°{estimated_pages}é¡µï¼Œé™åˆ¶è·Ÿéš{max_pages}é¡µ")
        else:
            max_pages = 3
            reasoning_parts.append(f"æ£€æµ‹åˆ°{estimated_pages}é¡µï¼Œä¿å®ˆè·Ÿéš{max_pages}é¡µ")
    else:
        max_pages = 1
        reasoning_parts.append("æœªæ£€æµ‹åˆ°åˆ†é¡µï¼Œä»…åˆ†æé¦–é¡µ")
    
    # 4. æ ¹æ®é¡µé¢è§„æ¨¡è°ƒæ•´
    if page_analysis['total_links'] > 200:
        # å¯¹äºå­¦æœ¯é¡µé¢ï¼Œå°¤å…¶æ˜¯æ•™æˆå¯†åº¦é«˜çš„é¡µé¢ï¼Œä¸é™åˆ¶é“¾æ¥æ•°
        if page_analysis['page_type'] in ['nyu_steinhardt', 'faculty_list', 'college'] and professor_density > 0.5:
            reasoning_parts.append("å¤§å‹å­¦æœ¯é¡µé¢ä¸”æ•™æˆå¯†åº¦é«˜ï¼Œä¸é™åˆ¶åˆ†æèŒƒå›´")
        else:
            max_links = min(max_links, 40)  # æ™®é€šå¤§å‹é¡µé¢ï¼Œæ§åˆ¶åˆ†æèŒƒå›´
            reasoning_parts.append("å¤§å‹é¡µé¢ï¼Œæ§åˆ¶åˆ†æèŒƒå›´")
    elif page_analysis['total_links'] < 50:
        max_links = max(max_links, 15)  # å°å‹é¡µé¢ï¼Œç¡®ä¿è¶³å¤Ÿåˆ†æ
        reasoning_parts.append("å°å‹é¡µé¢ï¼Œç¡®ä¿è¶³å¤Ÿåˆ†æ")
    
    # 5. å®‰å…¨è¾¹ç•Œ
    max_links = max(10, min(100, max_links))  # é™åˆ¶åœ¨10-100ä¹‹é—´
    max_pages = max(1, min(10, max_pages))    # é™åˆ¶åœ¨1-10ä¹‹é—´
    
    reasoning = "; ".join(reasoning_parts) if reasoning_parts else "ä½¿ç”¨é»˜è®¤å‚æ•°"
    
    return {
        'max_links': max_links,
        'max_pages': max_pages,
        'reasoning': reasoning,
        'page_type': page_analysis['page_type'],
        'professor_density': professor_density,
        'pagination_detected': pagination_info['has_pagination']
    }


def clean_faculty_url(url: str) -> str:
    """æ¸…ç†æ•™å¸ˆé¡µé¢URLï¼Œå»é™¤ç­›é€‰å‚æ•°"""
    import urllib.parse
    
    # è§£æURL
    parsed = urllib.parse.urlparse(url)
    
    # ç‰¹æ®Šå¤„ç†NYU Steinhardtç±»å‹çš„URL
    if 'steinhardt.nyu.edu' in parsed.netloc and 'faculty' in parsed.path:
        # å»é™¤æŸ¥è¯¢å‚æ•°ï¼Œä¿ç•™åŸºç¡€çš„facultyé¡µé¢
        clean_url = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
        if clean_url.endswith('faculty'):
            return clean_url
        # å¦‚æœURLåŒ…å«å…¶ä»–è·¯å¾„ï¼Œä¿ç•™åˆ°facultyä¸ºæ­¢
        path_parts = parsed.path.split('/')
        try:
            faculty_index = path_parts.index('faculty')
            clean_path = '/'.join(path_parts[:faculty_index+1])
            return f"{parsed.scheme}://{parsed.netloc}{clean_path}"
        except ValueError:
            pass
    
    return url


def adaptive_analysis_with_intelligent_params(
    start_url: str, 
    api_key: str, 
    max_workers: int = 5,
    use_intelligent_params: bool = True
) -> List[Dict[str, any]]:
    """ä½¿ç”¨æ™ºèƒ½å‚æ•°æ¨èçš„è‡ªé€‚åº”åˆ†æ
    
    Args:
        start_url: èµ·å§‹URL
        api_key: OpenAI APIå¯†é’¥
        max_workers: æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
        use_intelligent_params: æ˜¯å¦ä½¿ç”¨æ™ºèƒ½å‚æ•°æ¨è
        
    Returns:
        åˆ†æç»“æœåˆ—è¡¨
    """
    logger.info(f"å¼€å§‹è‡ªé€‚åº”åˆ†æ: {start_url}")
    
    # æ¸…ç†URLï¼Œå»é™¤ç­›é€‰å‚æ•°
    clean_url = clean_faculty_url(start_url)
    if clean_url != start_url:
        logger.info(f"URLå·²æ¸…ç†: {start_url} -> {clean_url}")
        start_url = clean_url
    
    # 1. è·å–æ™ºèƒ½å‚æ•°æ¨è
    if use_intelligent_params:
        logger.info("ğŸ§  è·å–æ™ºèƒ½å‚æ•°æ¨è...")
        param_recommendations = intelligent_parameter_estimation(start_url)
        max_links = param_recommendations['max_links']
        max_pages = param_recommendations['max_pages']
        logger.info(f"æ™ºèƒ½æ¨è - é“¾æ¥æ•°: {max_links}, é¡µé¢æ•°: {max_pages}")
        logger.info(f"æ¨èåŸå› : {param_recommendations['reasoning']}")
    else:
        max_links = 30
        max_pages = 3
        logger.info("ä½¿ç”¨é»˜è®¤å‚æ•°")
    
    # 2. æ‰§è¡Œåˆæ­¥åˆ†æ
    logger.info(f"ğŸš€ å¼€å§‹åˆ†æ (é“¾æ¥: {max_links}, é¡µé¢: {max_pages})")
    df_results = analyze_webpage_links(start_url, api_key, max_links, max_workers, max_pages)
    results = df_results.to_dict('records')
    
    # 3. åˆ†æåˆæ­¥ç»“æœå¹¶å†³å®šæ˜¯å¦éœ€è¦è°ƒæ•´
    if use_intelligent_params and results:
        adjustment = analyze_results_and_adjust_params(results, max_links, max_pages)
        
        if adjustment['should_adjust']:
            logger.info(f"ğŸ“Š ç»“æœåˆ†æ: {adjustment['reason']}")
            logger.info(f"è°ƒæ•´å‚æ•° - æ–°é“¾æ¥æ•°: {adjustment['new_max_links']}, æ–°é¡µé¢æ•°: {adjustment['new_max_pages']}")
            
            # æ‰§è¡Œè°ƒæ•´åçš„åˆ†æ
            additional_df = analyze_webpage_links(
                start_url, 
                api_key, 
                adjustment['new_max_links'], 
                max_workers,
                adjustment['new_max_pages']
            )
            additional_results = additional_df.to_dict('records')
            
            # åˆå¹¶ç»“æœå¹¶å»é‡
            all_results = results + additional_results
            seen_urls = set()
            deduplicated_results = []
            
            for result in all_results:
                url = result.get('url', '')
                if url not in seen_urls:
                    seen_urls.add(url)
                    deduplicated_results.append(result)
            
            logger.info(f"åˆå¹¶ç»“æœ: {len(results)} + {len(additional_results)} = {len(deduplicated_results)} (å»é‡å)")
            results = deduplicated_results
    
    return results


def analyze_results_and_adjust_params(
    results: List[Dict[str, any]], 
    current_max_links: int, 
    current_max_pages: int
) -> Dict[str, any]:
    """åˆ†æåˆæ­¥ç»“æœå¹¶å†³å®šå‚æ•°è°ƒæ•´ç­–ç•¥"""
    
    professor_results = [r for r in results if r.get('Is Professor Page') == 'Yes']
    professor_count = len(professor_results)
    total_results = len(results)
    
    adjustment = {
        'should_adjust': False,
        'reason': '',
        'new_max_links': current_max_links,
        'new_max_pages': current_max_pages
    }
    
    # è®¡ç®—æ•™æˆå‘ç°ç‡
    professor_rate = professor_count / total_results if total_results > 0 else 0
    
    # åˆ†ææ˜¯å¦éœ€è¦è°ƒæ•´å‚æ•°
    if professor_count == 0:
        # æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ•™æˆï¼Œå¤§å¹…å¢åŠ æœç´¢èŒƒå›´
        adjustment.update({
            'should_adjust': True,
            'reason': f'æœªå‘ç°æ•™æˆï¼Œå¢åŠ æœç´¢èŒƒå›´',
            'new_max_links': min(current_max_links * 2, 100),
            'new_max_pages': min(current_max_pages + 2, 8)
        })
    elif professor_count < 20 and professor_rate > 0.4:
        # æ•™æˆå‘ç°ç‡é«˜ä½†æ•°é‡å°‘ï¼Œå¯èƒ½æ˜¯å‚æ•°å¤ªä¿å®ˆï¼Œå¢åŠ æœç´¢èŒƒå›´
        adjustment.update({
            'should_adjust': True,
            'reason': f'æ•™æˆå‘ç°ç‡é«˜({professor_rate:.1%})ä½†æ•°é‡å°‘({professor_count}ä¸ª)ï¼Œå¢åŠ æœç´¢èŒƒå›´',
            'new_max_links': min(current_max_links * 2, 100),
            'new_max_pages': min(current_max_pages + 1, 6)
        })
    elif professor_count < 5 and professor_rate < 0.2:
        # æ•™æˆæ•°é‡è¾ƒå°‘ä¸”å‘ç°ç‡ä½ï¼Œé€‚åº¦å¢åŠ æœç´¢
        adjustment.update({
            'should_adjust': True,
            'reason': f'æ•™æˆæ•°é‡å°‘({professor_count}ä¸ª)ä¸”å‘ç°ç‡ä½({professor_rate:.1%})ï¼Œå¢åŠ æœç´¢',
            'new_max_links': min(current_max_links + 20, 80),
            'new_max_pages': min(current_max_pages + 1, 6)
        })
    elif professor_rate > 0.8 and current_max_links > 60:
        # å‘ç°ç‡å¾ˆé«˜ä¸”å·²ç»æœç´¢äº†å¾ˆå¤šé“¾æ¥ï¼Œå‚æ•°å·²è¶³å¤Ÿ
        adjustment.update({
            'should_adjust': False,
            'reason': f'å‘ç°ç‡å¾ˆé«˜({professor_rate:.1%})ä¸”æœç´¢èŒƒå›´å……è¶³ï¼Œå½“å‰å‚æ•°å·²è¶³å¤Ÿ'
        })
    
    logger.info(f"ç»“æœåˆ†æ - æ•™æˆ: {professor_count}/{total_results} ({professor_rate:.1%})")
    
    return adjustment


def test_nyu_steinhardt_fixes():
    """æµ‹è¯•NYU Steinhardté¡µé¢çš„ä¿®å¤æ•ˆæœ"""
    # åˆ›å»ºæµ‹è¯•ä¼šè¯
    session = create_session()
    
    # æµ‹è¯•URL
    test_url = "https://steinhardt.nyu.edu/about/faculty"
    
    logger.info("ğŸ§ª æµ‹è¯•NYU Steinhardtä¿®å¤æ•ˆæœ...")
    
    try:
        # è·å–é¡µé¢å†…å®¹
        response = robust_web_request(session, test_url)
        soup = BeautifulSoup(response.text, "html.parser")
        
        # åˆ†æé¡µé¢ç»“æ„
        page_structure = analyze_page_structure(soup)
        
        # æµ‹è¯•å‡ ä¸ªæ ·æœ¬é“¾æ¥
        test_cases = [
            "Dianna Heldman",
            "Dave Pietro", 
            "Juan Pablo Bello",
            "Marilyn Nonken"
        ]
        
        logger.info("æµ‹è¯•æ•™æˆå§“åé“¾æ¥è¯„åˆ†:")
        for name in test_cases:
            # åˆ›å»ºæ¨¡æ‹Ÿçš„anchorå…ƒç´ 
            mock_html = f'<a href="/people/{name.lower().replace(" ", "-")}">{name}</a>'
            mock_soup = BeautifulSoup(mock_html, 'html.parser')
            mock_anchor = mock_soup.find('a')
            href = f"https://steinhardt.nyu.edu/people/{name.lower().replace(' ', '-')}"
            
            score = calculate_link_score(mock_anchor, href, page_structure, test_url)
            logger.info(f"  {name}: {score} åˆ†")
            
    except Exception as e:
        logger.error(f"æµ‹è¯•å¤±è´¥: {e}")
    
    logger.info("âœ… NYU Steinhardtä¿®å¤æµ‹è¯•å®Œæˆ")


# Example usage
if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(
        description="Analyze a website to find professor pages and research interests."
    )
    parser.add_argument("url", help="Starting URL to analyze")
    parser.add_argument("--api-key", required=True, help="API key for OpenAI services")
    parser.add_argument(
        "--max-links", type=int, default=30, help="Maximum number of links to analyze"
    )
    parser.add_argument(
        "--max-pages",
        type=int,
        default=3,
        help="Maximum number of pages to follow when detecting pagination",
    )
    parser.add_argument(
        "--workers", type=int, default=5, help="Number of worker threads"
    )
    parser.add_argument(
        "--output", default="professor_analysis_results.csv", help="Output CSV filename"
    )
    parser.add_argument(
        "--test-fixes", action="store_true", help="Test NYU Steinhardt fixes"
    )
    args = parser.parse_args()
    
    # å¦‚æœæ˜¯æµ‹è¯•æ¨¡å¼ï¼Œè¿è¡Œæµ‹è¯•å¹¶é€€å‡º
    if args.test_fixes:
        test_nyu_steinhardt_fixes()
        exit(0)

    # Run the analysis
    results_df = analyze_webpage_links(
        args.url,
        args.api_key,
        max_links=args.max_links,
        max_workers=args.workers,
        max_pages=args.max_pages,
    )

    # Save full results to CSV
    results_df.to_csv(args.output, index=False)
    logger.info(f"Results saved to {args.output}")

    # Display professor pages
    professor_pages = results_df[results_df["Is Professor Page"] == "Yes"]
    if not professor_pages.empty:
        # Save professor-only results to a separate file
        professor_output = args.output.replace(".csv", "_professors_only.csv")
        professor_pages.to_csv(professor_output, index=False)

        # Print professor pages to console
        print("\nProfessor Pages Found:")
        print(professor_pages.to_string())
    else:
        print("\nNo professor pages found.")
"""
python main.py https://journalism.uiowa.edu/people --max-links 500 --workers 5
"""
