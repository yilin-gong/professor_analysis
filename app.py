import streamlit as st
import pandas as pd
from main import (
    analyze_webpage_links,
    adaptive_analysis_with_intelligent_params,
    intelligent_parameter_estimation,
    get_client
)
import os
import tempfile
import concurrent.futures
import time
import requests
import re
from bs4 import BeautifulSoup
import logging
import json

# è®¾ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

st.set_page_config(page_title="æ•™æˆç ”ç©¶å…´è¶£åˆ†æå™¨ ğŸ“", layout="wide")

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€å˜é‡
if "professor_results" not in st.session_state:
    st.session_state.professor_results = None
if "api_key" not in st.session_state:
    st.session_state.api_key = None
if "analysis_results" not in st.session_state:
    st.session_state.analysis_results = None
if "similarity_results" not in st.session_state:
    st.session_state.similarity_results = None


def render_keyword_tag(keyword: str, highlight: bool = False) -> str:
    """æ¸²æŸ“å…³é”®è¯æ ‡ç­¾ï¼Œæ”¯æŒé«˜äº®æ˜¾ç¤º"""
    base_style = "display: inline-block; margin: 2px; padding: 4px 8px; border-radius: 12px; font-size: 12px; font-weight: 500;"
    
    if highlight:
        # åŒ¹é…çš„å…³é”®è¯ç”¨é«˜äº®é¢œè‰²
        style = base_style + "background: linear-gradient(45deg, #FF6B6B, #4ECDC4); color: white; box-shadow: 0 2px 4px rgba(0,0,0,0.2);"
    else:
        # æ™®é€šå…³é”®è¯ç”¨ç°è‰²
        style = base_style + "background: #f0f2f6; color: #262730; border: 1px solid #e6eaed;"
    
    return f"<span style='{style}'>{keyword}</span>"


def calculate_similarity(prof_interests, user_interests, api_key):
    """ä½¿ç”¨LLMè®¡ç®—æ•™æˆç ”ç©¶å…´è¶£ä¸ç”¨æˆ·å…´è¶£çš„ç›¸ä¼¼åº¦"""
    try:
        # åˆ›å»ºå®¢æˆ·ç«¯
        current_client = get_client(api_key)

        llm_response = current_client.chat.completions.create(
            model="doubao-seed-1-6-250615",
            messages=[
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­¦æœ¯åŒ¹é…ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ†ææ•™æˆçš„ç ”ç©¶å…´è¶£ä¸ç”¨æˆ·çš„ç ”ç©¶å…´è¶£ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼Œç»™å‡º0-100çš„åˆ†æ•°å’Œè¯¦ç»†è§£é‡Šã€‚",
                },
                {
                    "role": "user",
                    "content": f"è¯·åˆ†æä»¥ä¸‹æ•™æˆçš„ç ”ç©¶å…´è¶£ä¸ç”¨æˆ·çš„ç ”ç©¶å…´è¶£ä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚ç»™å‡º0-100çš„ç›¸ä¼¼åº¦åˆ†æ•°å’Œè¯¦ç»†è§£é‡Šã€‚\n\næ•™æˆç ”ç©¶å…´è¶£: {prof_interests}\n\nç”¨æˆ·ç ”ç©¶å…´è¶£: {user_interests}",
                },
            ],
        )
        return llm_response.choices[0].message.content.strip()
    except Exception as e:
        return f"åˆ†æç›¸ä¼¼åº¦æ—¶å‡ºé”™: {str(e)}"


def extract_structured_similarity_data(similarity_text):
    """ä»ç›¸ä¼¼åº¦åˆ†ææ–‡æœ¬ä¸­æå–ç»“æ„åŒ–æ•°æ®"""
    try:
        # æ¸…ç†å“åº”æ–‡æœ¬ï¼Œç§»é™¤å¯èƒ½çš„markdownæ ‡è®°
        cleaned_text = similarity_text.strip()
        if cleaned_text.startswith('```json'):
            cleaned_text = cleaned_text[7:]
        if cleaned_text.endswith('```'):
            cleaned_text = cleaned_text[:-3]
        cleaned_text = cleaned_text.strip()
        
        # å°è¯•è§£æJSON
        try:
            result = json.loads(cleaned_text)
            
            # éªŒè¯JSONç»“æ„
            if validate_similarity_structure(result):
                return {
                    'success': True,
                    'data': result,
                    'overall_score': result.get('overall_similarity', 0),
                    'dimension_scores': result.get('dimension_scores', {}),
                    'matched_keywords': result.get('matched_keywords', []),
                    'reasoning': result.get('reasoning', {}),
                    'confidence': result.get('confidence', 0.5)
                }
        except json.JSONDecodeError:
            pass
        
        # JSONè§£æå¤±è´¥ï¼Œä½¿ç”¨fallbackæœºåˆ¶
        fallback_result = extract_fallback_similarity_data(similarity_text)
        return {
            'success': False,
            'data': fallback_result,
            'overall_score': fallback_result.get('overall_similarity', 0),
            'dimension_scores': {},
            'matched_keywords': [],
            'reasoning': {'overall': similarity_text},
            'confidence': 0.3
        }
        
    except Exception as e:
        # å‡ºé”™æ—¶è¿”å›é»˜è®¤å€¼
        return {
            'success': False,
            'data': {},
            'overall_score': 0,
            'dimension_scores': {},
            'matched_keywords': [],
            'reasoning': {'overall': f'è§£æé”™è¯¯: {str(e)}'},
            'confidence': 0.0
        }


def validate_similarity_structure(data):
    """éªŒè¯ç›¸ä¼¼åº¦åˆ†æç»“æœçš„JSONç»“æ„æ˜¯å¦æ­£ç¡®"""
    if not isinstance(data, dict):
        return False
    
    required_keys = ['overall_similarity', 'dimension_scores', 'reasoning']
    if not all(key in data for key in required_keys):
        return False
    
    # éªŒè¯åˆ†æ•°èŒƒå›´
    overall = data.get('overall_similarity')
    if not isinstance(overall, (int, float)) or not (0 <= overall <= 100):
        return False
    
    # éªŒè¯ç»´åº¦åˆ†æ•°
    dimension_scores = data.get('dimension_scores', {})
    if not isinstance(dimension_scores, dict):
        return False
    
    expected_dimensions = ['research_topics', 'research_methods', 'theoretical_framework', 
                          'application_domains', 'keyword_matching']
    
    for dim in expected_dimensions:
        score = dimension_scores.get(dim)
        if score is not None and (not isinstance(score, (int, float)) or not (0 <= score <= 100)):
            return False
    
    # éªŒè¯ç½®ä¿¡åº¦
    confidence = data.get('confidence')
    if confidence is not None and (not isinstance(confidence, (int, float)) or not (0 <= confidence <= 1)):
        return False
    
    return True


def extract_fallback_similarity_data(similarity_text):
    """å½“JSONè§£æå¤±è´¥æ—¶çš„å¤‡ç”¨æ•°æ®æå–æœºåˆ¶"""
    import re
    
    # å°è¯•æŸ¥æ‰¾åˆ†æ•°æ¨¡å¼
    score_patterns = [
        r"ç›¸ä¼¼åº¦[ï¼š:]\s*(\d+)",
        r"æ€»ä½“ç›¸ä¼¼åº¦[ï¼š:]\s*(\d+)",
        r"overall_similarity[\"'\s]*[ï¼š:]\s*(\d+)",
        r"(\d+)\s*åˆ†",
        r"(\d+)/100"
    ]
    
    extracted_score = 0
    for pattern in score_patterns:
        matches = re.findall(pattern, similarity_text, re.IGNORECASE)
        if matches:
            try:
                score = int(matches[0])
                if 0 <= score <= 100:
                    extracted_score = score
                    break
            except ValueError:
                continue
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆåˆ†æ•°ï¼Œå°è¯•æå–æ‰€æœ‰æ•°å­—å¹¶é€‰æ‹©æœ€åˆç†çš„
    if extracted_score == 0:
        all_numbers = re.findall(r'\b(\d{1,3})\b', similarity_text)
        valid_scores = [int(num) for num in all_numbers if 0 <= int(num) <= 100]
        if valid_scores:
            # é€‰æ‹©ä¸­ä½æ•°ä½œä¸ºæœ€å¯èƒ½çš„åˆ†æ•°
            valid_scores.sort()
            extracted_score = valid_scores[len(valid_scores) // 2]
    
    return {
        'overall_similarity': extracted_score,
        'dimension_scores': {},
        'matched_keywords': [],
        'reasoning': {'overall': similarity_text},
        'confidence': 0.3
    }


def calculate_advanced_similarity(prof_interests, prof_keywords, user_interests, api_key):
    """ä½¿ç”¨LLMè¿›è¡Œå¤šç»´åº¦ç ”ç©¶å…´è¶£ç›¸ä¼¼åº¦åˆ†æ"""
    try:
        # åˆ›å»ºå®¢æˆ·ç«¯
        current_client = get_client(api_key)

        # å¤„ç†æ•™æˆå…³é”®è¯
        keywords_str = ", ".join(prof_keywords) if prof_keywords and isinstance(prof_keywords, list) else "æ— å…³é”®è¯"

        # æ„å»ºç»“æ„åŒ–æç¤ºè¯
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­¦æœ¯åŒ¹é…åˆ†æä¸“å®¶ã€‚ä½ éœ€è¦ä»å¤šä¸ªç»´åº¦åˆ†ææ•™æˆçš„ç ”ç©¶å…´è¶£ä¸ç”¨æˆ·ç ”ç©¶å…´è¶£çš„åŒ¹é…åº¦ã€‚

è¯·æŒ‰ç…§ä»¥ä¸‹5ä¸ªç»´åº¦è¿›è¡Œè¯„åˆ†ï¼ˆæ¯ä¸ªç»´åº¦0-100åˆ†ï¼‰ï¼š

1. **ç ”ç©¶ä¸»é¢˜ç›¸ä¼¼åº¦**: ç ”ç©¶çš„æ ¸å¿ƒè¯é¢˜ã€é—®é¢˜é¢†åŸŸçš„é‡å ç¨‹åº¦
2. **ç ”ç©¶æ–¹æ³•åŒ¹é…åº¦**: ç ”ç©¶æ–¹æ³•è®ºã€æŠ€æœ¯æ‰‹æ®µã€åˆ†æå·¥å…·çš„ç›¸ä¼¼æ€§  
3. **ç†è®ºæ¡†æ¶é‡å åº¦**: ç†è®ºåŸºç¡€ã€å­¦ç§‘èƒŒæ™¯ã€æ¦‚å¿µæ¡†æ¶çš„å¥‘åˆåº¦
4. **åº”ç”¨é¢†åŸŸå¥‘åˆåº¦**: å®é™…åº”ç”¨åœºæ™¯ã€ç›®æ ‡ç¾¤ä½“ã€è§£å†³é—®é¢˜çš„ç›¸ä¼¼æ€§
5. **å…³é”®è¯ç²¾ç¡®åŒ¹é…**: å…·ä½“æœ¯è¯­ã€ä¸“ä¸šè¯æ±‡çš„ç›´æ¥åŒ¹é…ç¨‹åº¦

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ç»“æœï¼š
{
    "overall_similarity": æ•´ä½“ç›¸ä¼¼åº¦åˆ†æ•°(0-100),
    "dimension_scores": {
        "research_topics": ç ”ç©¶ä¸»é¢˜åˆ†æ•°(0-100),
        "research_methods": ç ”ç©¶æ–¹æ³•åˆ†æ•°(0-100), 
        "theoretical_framework": ç†è®ºæ¡†æ¶åˆ†æ•°(0-100),
        "application_domains": åº”ç”¨é¢†åŸŸåˆ†æ•°(0-100),
        "keyword_matching": å…³é”®è¯åŒ¹é…åˆ†æ•°(0-100)
    },
    "matched_keywords": ["åŒ¹é…çš„å…³é”®è¯1", "åŒ¹é…çš„å…³é”®è¯2"],
    "reasoning": {
        "strengths": "åŒ¹é…ä¼˜åŠ¿çš„å…·ä½“è¯´æ˜",
        "gaps": "å­˜åœ¨å·®è·çš„å…·ä½“åˆ†æ", 
        "overall": "ç»¼åˆè¯„ä»·å’Œå»ºè®®"
    },
    "confidence": ç½®ä¿¡åº¦(0.0-1.0)
}

æ³¨æ„ï¼š
- æ‰€æœ‰åˆ†æ•°å¿…é¡»æ˜¯0-100ä¹‹é—´çš„æ•´æ•°
- æ•´ä½“ç›¸ä¼¼åº¦åº”è¯¥æ˜¯å„ç»´åº¦åˆ†æ•°çš„åŠ æƒå¹³å‡
- ç½®ä¿¡åº¦åæ˜ åˆ†æçš„å¯é ç¨‹åº¦
- åŒ¹é…å…³é”®è¯åº”ä»æ•™æˆå…³é”®è¯ä¸­é€‰æ‹©ä¸ç”¨æˆ·å…´è¶£ç›¸å…³çš„è¯æ±‡"""

        user_prompt = f"""è¯·åˆ†æä»¥ä¸‹æ•™æˆä¸ç”¨æˆ·çš„ç ”ç©¶å…´è¶£åŒ¹é…åº¦ï¼š

**æ•™æˆç ”ç©¶å…´è¶£:**
{prof_interests}

**æ•™æˆå…³é”®è¯:**  
{keywords_str}

**ç”¨æˆ·ç ”ç©¶å…´è¶£:**
{user_interests}

è¯·æŒ‰ç…§è¦æ±‚çš„JSONæ ¼å¼è¿›è¡Œå¤šç»´åº¦åˆ†æã€‚"""

        llm_response = current_client.chat.completions.create(
            model="doubao-seed-1-6-250615",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.1  # é™ä½æ¸©åº¦ä»¥è·å¾—æ›´ä¸€è‡´çš„ç»“æœ
        )
        
        return llm_response.choices[0].message.content.strip()
    except Exception as e:
        return f"å¤šç»´åº¦ç›¸ä¼¼åº¦åˆ†ææ—¶å‡ºé”™: {str(e)}"


# æ·»åŠ ä¸€ä¸ªå‡½æ•°æ¥æ˜¾ç¤ºç»“æœï¼Œé¿å…ä»£ç é‡å¤
def display_results(results_df):
    """æ˜¾ç¤ºç›¸ä¼¼åº¦ç»“æœ"""
    if results_df is None or len(results_df) == 0:
        st.warning("âš ï¸ æ²¡æœ‰ç»“æœå¯æ˜¾ç¤º")
        return

    for _, row in results_df.iterrows():
        with st.expander(f"ğŸ”— ç›¸ä¼¼åº¦ {row['Score']}åˆ† - {row.get('Professor Name', 'Unknown')}"):
            col1, col2 = st.columns([2, 1])
            
            with col1:
                st.markdown("#### ğŸ‘¨â€ğŸ« æ•™æˆä¿¡æ¯")
                st.write(f"**å§“å:** {row.get('Professor Name', 'N/A')}")
                st.write(f"**èŒä½:** {row.get('Title', 'N/A')}")
                st.write(f"**é™¢ç³»:** {row.get('Department', 'N/A')}")
                st.write(f"**ç½‘å€:** {row['URL']}")
                
                st.markdown("#### ğŸ“– ç ”ç©¶å…´è¶£")
                st.write(row["Research Interests"])
                
                st.markdown("#### ğŸ“Š ç›¸ä¼¼åº¦åˆ†æ")
                st.write(row["Similarity Analysis"])
            
            with col2:
                st.markdown("#### ğŸ·ï¸ å…³é”®è¯")
                keywords = row.get('Keywords', [])
                if keywords and isinstance(keywords, list):
                    for keyword in keywords:
                        st.markdown(render_keyword_tag(keyword), unsafe_allow_html=True)
                else:
                    st.write("æ— å…³é”®è¯")
                
                st.markdown("#### ğŸ“ ç›¸å…³é“¾æ¥")
                additional_urls = row.get('Additional URLs', [])
                if additional_urls and isinstance(additional_urls, list):
                    for url in additional_urls[:3]:  # é™åˆ¶æ˜¾ç¤º3ä¸ªç›¸å…³é“¾æ¥
                        st.markdown(f"[ç›¸å…³é¡µé¢]({url})")
                else:
                    st.write("æ— ç›¸å…³é“¾æ¥")
                
                if row.get('Confidence Score'):
                    st.markdown("#### ğŸ¯ ç½®ä¿¡åº¦")
                    confidence = float(row.get('Confidence Score', 0))
                    st.progress(confidence)
                    st.write(f"{confidence:.1%}")


def display_professor_results(results: list, key_prefix: str = "default"):
    """æ˜¾ç¤ºæ•™æˆåˆ†æç»“æœ"""
    if not results:
        st.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°æ•™æˆé¡µé¢")
        return
        
    # è½¬æ¢ä¸ºDataFrame
    df = pd.DataFrame(results)
    
    # ç°åœ¨resultsä¸­åªåŒ…å«æ•™æˆé¡µé¢ï¼Œä¸éœ€è¦è¿‡æ»¤
    professor_results = results
    
    st.subheader(f"ğŸ‘¨â€ğŸ« æ‰¾åˆ°çš„æ•™æˆé¡µé¢ ({len(professor_results)}ä½)")
    
    # æ·»åŠ è¿‡æ»¤å’Œæ’åºé€‰é¡¹ - ä½¿ç”¨åŠ¨æ€key
    col1, col2, col3 = st.columns(3)
    with col1:
        confidence_filter = st.slider("æœ€ä½ç½®ä¿¡åº¦", 0.0, 1.0, 0.0, 0.1, key=f"confidence_filter_{key_prefix}")
    with col2:
        sort_by = st.selectbox("æ’åºæ–¹å¼", ["ç½®ä¿¡åº¦", "æ•™æˆå§“å", "é™¢ç³»"], key=f"sort_by_{key_prefix}")
    with col3:
        sort_order = st.selectbox("æ’åºé¡ºåº", ["é™åº", "å‡åº"], key=f"sort_order_{key_prefix}")
    
    # åº”ç”¨è¿‡æ»¤
    filtered_results = [r for r in professor_results if r.get('Confidence Score', 0) >= confidence_filter]
    
    # åº”ç”¨æ’åº
    if sort_by == "ç½®ä¿¡åº¦":
        sort_key = lambda x: x.get('Confidence Score', 0)
    elif sort_by == "æ•™æˆå§“å":
        sort_key = lambda x: x.get('Professor Name', '')
    else:
        sort_key = lambda x: x.get('Department', '')
    
    ascending = (sort_order == "å‡åº")
    filtered_results = sorted(filtered_results, key=sort_key, reverse=not ascending)
    
    # æ˜¾ç¤ºç»“æœ
    for result in filtered_results:
        # ç»„è£…æ ‡é¢˜å¾½æ ‡
        title_badges = []
        if result.get('PhD Not Recruiting', False):
            title_badges.append("âŒ ä¸æ‹›åšå£«ç”Ÿ")
        if result.get('Insufficient Content', False):
            title_badges.append("âš ï¸ å†…å®¹ä¸è¶³")
        badges_str = ("  ".join(title_badges)) if title_badges else ""

        display_title = f"ğŸ‘¨â€ğŸ« {result.get('Professor Name', 'Unknown')} - {result.get('Title', '')}"
        if badges_str:
            display_title = f"{display_title}  |  {badges_str}"

        with st.expander(display_title):
            col1, col2 = st.columns([2, 1])
            
            with col1:
                st.markdown("#### ğŸ“ åŸºæœ¬ä¿¡æ¯")
                st.write(f"**å§“å:** {result.get('Professor Name', 'N/A')}")
                st.write(f"**èŒä½:** {result.get('Title', 'N/A')}")
                st.write(f"**é™¢ç³»:** {result.get('Department', 'N/A')}")
                st.write(f"**ç½‘å€:** {result.get('URL', 'N/A')}")
                
                st.markdown("#### ğŸ“– ç ”ç©¶å…´è¶£")
                research_interests = result.get('Research Interests', '')
                if research_interests:
                    st.write(research_interests)
                else:
                    st.write("æœªæ‰¾åˆ°ç ”ç©¶å…´è¶£ä¿¡æ¯")

                # é«˜äº®çŠ¶æ€åŒº
                if result.get('PhD Not Recruiting', False):
                    st.markdown("#### âŒ ä¸æ‹›åšå£«ç”Ÿ")
                    evidence = result.get('PhD Evidence', '')
                    if evidence:
                        st.info(evidence)
                    else:
                        st.info("é¡µé¢æ˜ç¡®è¡¨ç¤ºå½“å‰ä¸æ‹›æ”¶åšå£«ç”Ÿ")
                if result.get('Insufficient Content', False):
                    st.markdown("#### âš ï¸ å†…å®¹ä¸è¶³")
                    reasons = result.get('Insufficient Reasons', []) or []
                    if reasons:
                        # å°†æšä¸¾è½¬æ¢ä¸ºå¯è¯»æ ‡ç­¾
                        reason_map = {
                            'too_short_text': 'é¡µé¢æ–‡æœ¬è¿‡çŸ­',
                            'no_research_section': 'ç¼ºå°‘ç ”ç©¶ç›¸å…³æ¿å—',
                            'few_paragraphs_no_keywords': 'æ®µè½è¿‡å°‘ä¸”ç¼ºå°‘ç ”ç©¶å…³é”®è¯',
                            'mostly_contact_admin': 'ä¸»è¦ä¸ºè”ç³»/è¡Œæ”¿ä¿¡æ¯',
                            'exception_during_extraction': 'æå–è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸'
                        }
                        readable = [reason_map.get(r, r) for r in reasons]
                        st.warning("ï¼›".join(readable))
                    else:
                        st.warning("é¡µé¢å¯ç”¨ä¿¡æ¯ä¸è¶³")
            
            with col2:
                st.markdown("#### ğŸ·ï¸ å…³é”®è¯")
                keywords = result.get('Keywords', [])
                if keywords and isinstance(keywords, list) and len(keywords) > 0:
                    for keyword in keywords:
                        st.markdown(render_keyword_tag(keyword), unsafe_allow_html=True)
                else:
                    st.write("æ— å…³é”®è¯")
                
                st.markdown("#### ğŸ“ ç›¸å…³é“¾æ¥")
                additional_urls = result.get('Additional URLs', [])
                if additional_urls and isinstance(additional_urls, list) and len(additional_urls) > 0:
                    for url in additional_urls[:3]:  # é™åˆ¶æ˜¾ç¤º3ä¸ªç›¸å…³é“¾æ¥
                        st.markdown(f"[ç›¸å…³é¡µé¢]({url})")
                else:
                    st.write("æ— ç›¸å…³é“¾æ¥")
                
                st.markdown("#### ğŸ¯ ç½®ä¿¡åº¦")
                confidence = result.get('Confidence Score', 0)
                st.progress(confidence)
                st.write(f"{confidence:.1%}")
    
    # æä¾›ä¸‹è½½åŠŸèƒ½
    st.markdown("---")
    df_download = pd.DataFrame(professor_results)
    csv_data = df_download.to_csv(index=False)
    st.download_button(
        label="ğŸ“¥ ä¸‹è½½æ•™æˆä¿¡æ¯",
        data=csv_data,
        file_name="professor_analysis_results.csv",
        mime="text/csv",
        key=f"download_button_{key_prefix}"
    )

    # æ˜¾ç¤ºåˆ†æç»Ÿè®¡
    st.markdown("---")
    st.markdown("#### ğŸ“Š åˆ†æç»Ÿè®¡")
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("æ‰¾åˆ°æ•™æˆ", len(professor_results))
    with col2:
        avg_confidence = sum(r.get('Confidence Score', 0) for r in professor_results) / len(professor_results) if professor_results else 0
        st.metric("å¹³å‡ç½®ä¿¡åº¦", f"{avg_confidence:.1%}")
    with col3:
        with_keywords = sum(1 for r in professor_results if r.get('Keywords') and len(r.get('Keywords', [])) > 0)
        st.metric("å«å…³é”®è¯", f"{with_keywords}/{len(professor_results)}")


def display_advanced_similarity_results(results_df):
    """æ˜¾ç¤ºå¤šç»´åº¦ç›¸ä¼¼åº¦åˆ†æç»“æœ"""
    if results_df is None or len(results_df) == 0:
        st.warning("âš ï¸ æ²¡æœ‰ç»“æœå¯æ˜¾ç¤º")
        return

    # æŒ‰åˆ†æ•°æ’åº
    sorted_df = results_df.sort_values(by="Score", ascending=False)

    for _, row in sorted_df.iterrows():
        # è·å–ç»“æ„åŒ–æ•°æ®
        similarity_text = row.get("Similarity Analysis", "")
        similarity_data = extract_structured_similarity_data(similarity_text)
        
        # æ ‡é¢˜æ˜¾ç¤ºæ€»ä½“ç›¸ä¼¼åº¦å’Œç½®ä¿¡åº¦
        confidence_indicator = "ğŸ”¥" if similarity_data['confidence'] > 0.8 else "âœ…" if similarity_data['confidence'] > 0.5 else "âš ï¸"
        title = f"{confidence_indicator} ç›¸ä¼¼åº¦ {row['Score']}åˆ† - {row.get('Professor Name', 'Unknown')}"
        
        with st.expander(title):
            col1, col2 = st.columns([2, 1])
            
            with col1:
                st.markdown("#### ğŸ‘¨â€ğŸ« æ•™æˆä¿¡æ¯")
                st.write(f"**å§“å:** {row.get('Professor Name', 'N/A')}")
                st.write(f"**èŒä½:** {row.get('Title', 'N/A')}")
                st.write(f"**é™¢ç³»:** {row.get('Department', 'N/A')}")
                st.write(f"**ç½‘å€:** {row['URL']}")
                
                st.markdown("#### ğŸ“– ç ”ç©¶å…´è¶£")
                research_text = row["Research Interests"]
                matched_keywords = similarity_data.get('matched_keywords', [])
                if matched_keywords:
                    highlighted_research = highlight_matched_keywords(research_text, matched_keywords)
                    st.markdown(highlighted_research, unsafe_allow_html=True)
                else:
                    st.write(research_text)
                
                # æ˜¾ç¤ºå¤šç»´åº¦åˆ†æ
                if similarity_data['success'] and similarity_data['dimension_scores']:
                    st.markdown("#### ğŸ“Š å¤šç»´åº¦åŒ¹é…åˆ†æ")
                    
                    # å°è¯•æ˜¾ç¤ºé›·è¾¾å›¾
                    radar_fig = render_dimension_radar_chart(similarity_data['dimension_scores'])
                    if radar_fig:
                        # ä½¿ç”¨æ•™æˆURLä½œä¸ºå”¯ä¸€key
                        chart_key = f"radar_chart_{hash(row['URL'])}"
                        st.plotly_chart(radar_fig, use_container_width=True, key=chart_key)
                    else:
                        # å¦‚æœæ²¡æœ‰plotlyï¼Œä½¿ç”¨æ–‡æœ¬æ˜¾ç¤º
                        render_dimension_scores(similarity_data['dimension_scores'])
                    
                    # æ˜¾ç¤ºè¯¦ç»†æ¨ç†
                    reasoning = similarity_data['reasoning']
                    if isinstance(reasoning, dict):
                        if reasoning.get('strengths'):
                            st.markdown("**ğŸ¯ åŒ¹é…ä¼˜åŠ¿:**")
                            st.write(reasoning['strengths'])
                        if reasoning.get('gaps'):
                            st.markdown("**ğŸ” å¾…æ”¹è¿›ç‚¹:**")
                            st.write(reasoning['gaps'])
                        if reasoning.get('overall'):
                            st.markdown("**ğŸ’¡ ç»¼åˆè¯„ä»·:**")
                            st.write(reasoning['overall'])
                    else:
                        st.markdown("#### ğŸ“ è¯¦ç»†åˆ†æ")
                        st.write(similarity_data['reasoning'].get('overall', similarity_text))
                else:
                    st.markdown("#### ğŸ“ åˆ†æç»“æœ")
                    st.write(similarity_text)
            
            with col2:
                # æ˜¾ç¤ºåŒ¹é…å…³é”®è¯
                st.markdown("#### ğŸ”— åŒ¹é…å…³é”®è¯")
                matched_keywords = similarity_data.get('matched_keywords', [])
                if matched_keywords and isinstance(matched_keywords, list):
                    for keyword in matched_keywords:
                        st.markdown(render_keyword_tag(keyword, highlight=True), unsafe_allow_html=True)
                else:
                    st.write("æ— åŒ¹é…å…³é”®è¯")
                
                # æ˜¾ç¤ºæ•™æˆæ‰€æœ‰å…³é”®è¯
                st.markdown("#### ğŸ·ï¸ æ•™æˆå…³é”®è¯")
                prof_keywords = row.get('Keywords', [])
                if prof_keywords and isinstance(prof_keywords, list):
                    for keyword in prof_keywords:
                        is_matched = keyword in matched_keywords if matched_keywords else False
                        st.markdown(render_keyword_tag(keyword, highlight=is_matched), unsafe_allow_html=True)
                else:
                    st.write("æ— å…³é”®è¯")
                
                # æ˜¾ç¤ºç½®ä¿¡åº¦
                st.markdown("#### ğŸ¯ åˆ†æç½®ä¿¡åº¦")
                confidence = similarity_data.get('confidence', 0)
                st.progress(confidence)
                st.write(f"{confidence:.1%}")
                
                # æ˜¾ç¤ºç›¸å…³é“¾æ¥
                st.markdown("#### ğŸ“ ç›¸å…³é“¾æ¥")
                additional_urls = row.get('Additional URLs', [])
                if additional_urls and isinstance(additional_urls, list):
                    for i, url in enumerate(additional_urls[:3]):
                        st.markdown(f"[ç›¸å…³é¡µé¢ {i+1}]({url})")
                else:
                    st.write("æ— ç›¸å…³é“¾æ¥")


def render_dimension_scores(dimension_scores):
    """æ¸²æŸ“ç»´åº¦åˆ†æ•°æ˜¾ç¤º"""
    dimension_names = {
        'research_topics': 'ğŸ¯ ç ”ç©¶ä¸»é¢˜',
        'research_methods': 'ğŸ”¬ ç ”ç©¶æ–¹æ³•', 
        'theoretical_framework': 'ğŸ“š ç†è®ºæ¡†æ¶',
        'application_domains': 'ğŸŒ åº”ç”¨é¢†åŸŸ',
        'keyword_matching': 'ğŸ”— å…³é”®è¯åŒ¹é…'
    }
    
    # åˆ›å»ºä¸¤åˆ—å¸ƒå±€æ˜¾ç¤ºç»´åº¦åˆ†æ•°
    col1, col2 = st.columns(2)
    
    dimensions = list(dimension_scores.keys())
    for i, (dim_key, score) in enumerate(dimension_scores.items()):
        display_name = dimension_names.get(dim_key, dim_key)
        
        # äº¤æ›¿æ˜¾ç¤ºåœ¨ä¸¤åˆ—ä¸­
        with col1 if i % 2 == 0 else col2:
            # ä½¿ç”¨è¿›åº¦æ¡å’Œåˆ†æ•°æ˜¾ç¤º
            st.metric(label=display_name, value=f"{score}åˆ†")
            st.progress(score / 100.0)


def render_dimension_radar_chart(dimension_scores):
    """æ¸²æŸ“äº”ç»´åº¦é›·è¾¾å›¾"""
    try:
        import plotly.graph_objects as go
        import plotly.express as px
        
        # ç»´åº¦åç§°æ˜ å°„
        dimension_names = {
            'research_topics': 'ç ”ç©¶ä¸»é¢˜',
            'research_methods': 'ç ”ç©¶æ–¹æ³•', 
            'theoretical_framework': 'ç†è®ºæ¡†æ¶',
            'application_domains': 'åº”ç”¨é¢†åŸŸ',
            'keyword_matching': 'å…³é”®è¯åŒ¹é…'
        }
        
        # æå–åˆ†æ•°å’Œæ ‡ç­¾
        labels = []
        values = []
        for key, score in dimension_scores.items():
            labels.append(dimension_names.get(key, key))
            values.append(score)
        
        # åˆ›å»ºé›·è¾¾å›¾
        fig = go.Figure()
        
        fig.add_trace(go.Scatterpolar(
            r=values,
            theta=labels,
            fill='toself',
            fillcolor='rgba(59, 130, 246, 0.2)',
            line=dict(color='rgba(59, 130, 246, 0.8)', width=2),
            marker=dict(color='rgba(59, 130, 246, 1)', size=8),
            name='åŒ¹é…åº¦'
        ))
        
        fig.update_layout(
            polar=dict(
                radialaxis=dict(
                    visible=True,
                    range=[0, 100],
                    tickfont=dict(size=10),
                    gridcolor='rgba(0,0,0,0.1)'
                ),
                angularaxis=dict(
                    tickfont=dict(size=12)
                )
            ),
            showlegend=False,
            margin=dict(t=20, b=20, l=20, r=20),
            height=300,
            paper_bgcolor='rgba(0,0,0,0)',
            plot_bgcolor='rgba(0,0,0,0)'
        )
        
        return fig
    except ImportError:
        # å¦‚æœæ²¡æœ‰plotlyï¼Œè¿”å›None
        return None


def highlight_matched_keywords(text, keywords):
    """åœ¨æ–‡æœ¬ä¸­é«˜äº®åŒ¹é…çš„å…³é”®è¯"""
    if not keywords or not text:
        return text
    
    import re
    highlighted_text = text
    
    # å¯¹æ¯ä¸ªå…³é”®è¯è¿›è¡Œé«˜äº®å¤„ç†
    for keyword in keywords:
        if keyword and len(keyword.strip()) > 0:
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è¿›è¡Œå¤§å°å†™ä¸æ•æ„Ÿçš„åŒ¹é…
            pattern = re.compile(re.escape(keyword.strip()), re.IGNORECASE)
            highlighted_text = pattern.sub(
                f'<mark style="background-color: #FFE066; padding: 1px 3px; border-radius: 3px;">{keyword}</mark>',
                highlighted_text
            )
    
    return highlighted_text


def validate_similarity_scores(similarity_data):
    """éªŒè¯ç›¸ä¼¼åº¦åˆ†æ•°çš„ä¸€è‡´æ€§"""
    if not similarity_data.get('success', False):
        return True  # å¦‚æœè§£æå¤±è´¥ï¼Œè·³è¿‡éªŒè¯
    
    overall_score = similarity_data.get('overall_score', 0)
    dimension_scores = similarity_data.get('dimension_scores', {})
    
    # åŸºæœ¬èŒƒå›´æ£€æŸ¥
    if not (0 <= overall_score <= 100):
        return False
    
    # æ£€æŸ¥å„ç»´åº¦åˆ†æ•°
    for dim, score in dimension_scores.items():
        if not isinstance(score, (int, float)) or not (0 <= score <= 100):
            return False
    
    # ä¸€è‡´æ€§æ£€æŸ¥ï¼šæ€»ä½“åˆ†æ•°åº”è¯¥ä¸ç»´åº¦åˆ†æ•°ç›¸å…³
    if dimension_scores:
        avg_dimension_score = sum(dimension_scores.values()) / len(dimension_scores)
        # å…è®¸Â±20åˆ†çš„å·®å¼‚ï¼Œå› ä¸ºæ€»ä½“åˆ†æ•°å¯èƒ½æœ‰æƒé‡
        if abs(overall_score - avg_dimension_score) > 20:
            return False
    
    return True


def main():
    st.title("ğŸ“ æ•™æˆç ”ç©¶å…´è¶£åˆ†æå™¨")

    # APIè®¾ç½®åœ¨ä¾§è¾¹æ 
    with st.sidebar:
        st.header("âš™ï¸ è®¾ç½®")
        api_key = st.text_input("è¾“å…¥APIå¯†é’¥ï¼ˆå¿…å¡«ï¼‰", type="password")
        if api_key:
            st.session_state.api_key = api_key
            st.success("âœ… APIå¯†é’¥å·²è®¾ç½®")
        else:
            st.warning("âš ï¸ è¯·è¾“å…¥APIå¯†é’¥æ‰èƒ½ä½¿ç”¨åˆ†æåŠŸèƒ½")

    tab1, tab2 = st.tabs(["ğŸ” ç½‘ç«™åˆ†æ", "ğŸ”„ å…´è¶£åŒ¹é…"])

    with tab1:
        st.header("ğŸ“Š åˆ†æå¤§å­¦ç½‘ç«™ä¸Šçš„æ•™æˆé¡µé¢")

        # åˆ†ææ¨¡å¼é€‰æ‹©
        st.markdown("#### ğŸ¯ åˆ†ææ¨¡å¼")
        analysis_mode = st.radio(
            "é€‰æ‹©åˆ†ææ¨¡å¼:",
            [
                "ğŸ§  æ™ºèƒ½è‡ªé€‚åº”åˆ†æ (æ¨è)",
                "âš™ï¸ æ‰‹åŠ¨å‚æ•°è®¾ç½®",
                "âš¡ å¿«é€Ÿåˆ†æ (é»˜è®¤å‚æ•°)"
            ],
            help="æ™ºèƒ½æ¨¡å¼ä¼šè‡ªåŠ¨åˆ†æé¡µé¢ç‰¹å¾ï¼Œæ¨èæœ€ä¼˜å‚æ•°ï¼Œå¹¶æ ¹æ®ç»“æœåŠ¨æ€è°ƒæ•´æœç´¢ç­–ç•¥"
        )

        if analysis_mode == "ğŸ§  æ™ºèƒ½è‡ªé€‚åº”åˆ†æ (æ¨è)":
            st.info("ğŸ’¡ **æ™ºèƒ½æ¨¡å¼**: ç¨‹åºå°†è‡ªåŠ¨åˆ†æé¡µé¢ç‰¹å¾ï¼Œæ¨èæœ€ä¼˜å‚æ•°ï¼Œå¹¶æ ¹æ®ç»“æœåŠ¨æ€è°ƒæ•´æœç´¢ç­–ç•¥")
            use_intelligent_params = True
            show_manual_params = False
            
        elif analysis_mode == "âš™ï¸ æ‰‹åŠ¨å‚æ•°è®¾ç½®":
            st.info("ğŸ”§ **æ‰‹åŠ¨æ¨¡å¼**: æ‚¨å¯ä»¥è‡ªå®šä¹‰æ‰€æœ‰åˆ†æå‚æ•°")
            use_intelligent_params = False
            show_manual_params = True
            
        else:  # å¿«é€Ÿåˆ†æ
            st.info("âš¡ **å¿«é€Ÿæ¨¡å¼**: ä½¿ç”¨é¢„è®¾çš„é»˜è®¤å‚æ•°è¿›è¡Œåˆ†æ")
            use_intelligent_params = False
            show_manual_params = False

        # åªåœ¨éœ€è¦æ—¶æ˜¾ç¤ºå‚æ•°æ¨èåŠŸèƒ½
        if analysis_mode == "âš™ï¸ æ‰‹åŠ¨å‚æ•°è®¾ç½®":
            col1, col2 = st.columns([2, 1])

            with col1:
                start_url = st.text_input(
                    "è¾“å…¥èµ·å§‹URL", "https://journalism.uiowa.edu/people", key="start_url_manual"
                )

            with col2:
                st.markdown("#### ğŸ§  æ™ºèƒ½å‚æ•°æ¨è")
                if st.button("ğŸ” åˆ†æé¡µé¢å¹¶æ¨èå‚æ•°", key="analyze_params_button"):
                    if start_url:
                        with st.spinner("æ­£åœ¨åˆ†æé¡µé¢ç‰¹å¾..."):
                            try:
                                recommendations = intelligent_parameter_estimation(start_url)
                                
                                st.success("âœ… æ™ºèƒ½åˆ†æå®Œæˆ!")
                                st.info(f"**æ¨èåŸå› **: {recommendations['reasoning']}")
                                
                                # ä¿å­˜æ¨èå‚æ•°åˆ°session state
                                st.session_state.recommended_max_links = recommendations['max_links']
                                st.session_state.recommended_max_pages = recommendations['max_pages']
                                st.session_state.page_analysis = {
                                    'page_type': recommendations.get('page_type', 'unknown'),
                                    'professor_density': recommendations.get('professor_density', 0),
                                    'pagination_detected': recommendations.get('pagination_detected', False)
                                }
                                
                            except Exception as e:
                                st.error(f"æ™ºèƒ½åˆ†æå¤±è´¥: {str(e)}")
        else:
            start_url = st.text_input(
                "è¾“å…¥èµ·å§‹URL", "https://journalism.uiowa.edu/people", key="start_url_default"
            )

        # å‚æ•°è®¾ç½®éƒ¨åˆ† - åªåœ¨æ‰‹åŠ¨æ¨¡å¼ä¸‹æ˜¾ç¤º
        if show_manual_params:
            st.markdown("#### âš™ï¸ åˆ†æå‚æ•°è®¾ç½®")
            
            col3, col4, col5 = st.columns(3)
            
            with col3:
                # æ£€æŸ¥æ˜¯å¦æœ‰æ¨èå‚æ•°
                recommended_links = getattr(st.session_state, 'recommended_max_links', 30)
                max_links = st.number_input(
                    "æœ€å¤§åˆ†æé“¾æ¥æ•°", 
                    min_value=10, 
                    max_value=500, 
                    value=recommended_links, 
                    step=10,
                    help="æ™ºèƒ½æ¨èçš„é“¾æ¥æ•°ï¼Œå¯æ‰‹åŠ¨è°ƒæ•´",
                    key="max_links_manual"
                )
                
            with col4:
                recommended_pages = getattr(st.session_state, 'recommended_max_pages', 3)
                max_pages = st.number_input(
                    "æœ€å¤šè·Ÿéšé¡µæ•°", 
                    min_value=1, 
                    max_value=10, 
                    value=recommended_pages, 
                    step=1,
                    help="æ™ºèƒ½æ¨èçš„é¡µé¢æ•°ï¼Œå¯æ‰‹åŠ¨è°ƒæ•´",
                    key="max_pages_manual"
                )
                
            with col5:
                max_workers = st.number_input(
                    "å·¥ä½œçº¿ç¨‹æ•°", min_value=1, max_value=10, value=5, step=1, key="max_workers_manual"
                )
            
            # æ˜¾ç¤ºé¡µé¢åˆ†æç»“æœ
            if hasattr(st.session_state, 'page_analysis'):
                analysis = st.session_state.page_analysis
                
                st.markdown("#### ğŸ“Š é¡µé¢åˆ†æç»“æœ")
                
                col6, col7, col8 = st.columns(3)
                with col6:
                    page_type_display = {
                        'department': 'ğŸ›ï¸ ç³»çº§é¡µé¢',
                        'college': 'ğŸ« å­¦é™¢çº§é¡µé¢', 
                        'faculty_list': 'ğŸ‘¥ æ•™æˆåˆ—è¡¨',
                        'unknown': 'â“ æœªçŸ¥ç±»å‹'
                    }
                    st.metric("é¡µé¢ç±»å‹", page_type_display.get(analysis['page_type'], 'æœªçŸ¥'))
                    
                with col7:
                    density = analysis.get('professor_density', 0)
                    st.metric("æ•™æˆé“¾æ¥å¯†åº¦", f"{density:.1%}")
                    
                with col8:
                    pagination_status = "âœ… æ£€æµ‹åˆ°" if analysis.get('pagination_detected') else "âŒ æœªæ£€æµ‹åˆ°"
                    st.metric("åˆ†é¡µç»“æ„", pagination_status)
        else:
            # åœ¨æ™ºèƒ½æ¨¡å¼ä¸‹ä»ç„¶æä¾›åŸºæœ¬çš„çº¿ç¨‹æ•°è®¾ç½®
            max_workers = st.number_input(
                "å·¥ä½œçº¿ç¨‹æ•°", min_value=1, max_value=10, value=5, step=1,
                help="å¹¶å‘å¤„ç†çš„çº¿ç¨‹æ•°é‡", key="max_workers_auto"
            )

        if st.button("ğŸš€ å¼€å§‹åˆ†æ", key="start_analysis_button"):
            if not st.session_state.api_key:
                st.error("âŒ è¯·åœ¨ä¾§è¾¹æ è®¾ç½® OpenAI API å¯†é’¥")
            elif not start_url:
                st.error("âŒ è¯·è¾“å…¥èµ·å§‹URL")
            else:
                try:
                    with st.spinner("æ­£åœ¨åˆ†ææ•™æˆé¡µé¢..."):
                        progress_bar = st.progress(0)
                        
                        # æ ¹æ®åˆ†ææ¨¡å¼é€‰æ‹©ä¸åŒçš„å‡½æ•°
                        if analysis_mode == "ğŸ§  æ™ºèƒ½è‡ªé€‚åº”åˆ†æ (æ¨è)":
                            progress_bar.progress(20)
                            
                            results = adaptive_analysis_with_intelligent_params(
                                start_url, 
                                st.session_state.api_key, 
                                max_workers,
                                use_intelligent_params=True
                            )
                            
                        elif analysis_mode == "âš™ï¸ æ‰‹åŠ¨å‚æ•°è®¾ç½®":
                            progress_bar.progress(20)
                            
                            # ç¡®ä¿å‚æ•°å·²å®šä¹‰
                            if 'max_links' not in locals():
                                max_links = getattr(st.session_state, 'recommended_max_links', 30)
                            if 'max_pages' not in locals():
                                max_pages = getattr(st.session_state, 'recommended_max_pages', 3)
                                
                            results = analyze_webpage_links(
                                start_url, 
                                st.session_state.api_key, 
                                max_links, 
                                max_pages, 
                                max_workers
                            )
                            
                        else:  # å¿«é€Ÿåˆ†ææ¨¡å¼
                            progress_bar.progress(20)
                            
                            results = analyze_webpage_links(
                                start_url, 
                                st.session_state.api_key, 
                                30,  # é»˜è®¤é“¾æ¥æ•°
                                3,   # é»˜è®¤é¡µé¢æ•°
                                max_workers
                            )
                        
                        progress_bar.progress(100)
                        
                        if results:
                            st.success(f"âœ… åˆ†æå®Œæˆï¼æ‰¾åˆ° {len(results)} ä¸ªé“¾æ¥")
                            st.session_state.results = results
                            
                            # æ˜¾ç¤ºåˆ†ææ¨¡å¼çš„æ‰§è¡Œç»“æœ
                            if analysis_mode == "ğŸ§  æ™ºèƒ½è‡ªé€‚åº”åˆ†æ (æ¨è)":
                                professor_count = len([r for r in results if r.get('Is Professor Page') == 'Yes'])
                                st.info(f"ğŸ¯ **æ™ºèƒ½åˆ†æç»“æœ**: å‘ç° {professor_count} ä½æ•™æˆï¼Œåˆ†æäº† {len(results)} ä¸ªé“¾æ¥")
                            
                            # æ˜¾ç¤ºç»“æœ
                            display_professor_results(results, key_prefix="tab1_main")
                            
                        else:
                            st.warning("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•ç›¸å…³é“¾æ¥")
                            
                except Exception as e:
                    st.error(f"âŒ åˆ†æå¤±è´¥: {str(e)}")
                    logger.error(f"Analysis failed: {e}", exc_info=True)
                finally:
                    progress_bar.empty()

        # æ˜¾ç¤ºä¼šè¯ä¸­ä¿å­˜çš„ç»“æœ
        if hasattr(st.session_state, 'results') and st.session_state.results:
            st.subheader("ğŸ“‹ ä¸Šæ¬¡åˆ†æç»“æœ")
            display_professor_results(st.session_state.results, key_prefix="tab1_saved")

    with tab2:
        st.header("ğŸ”„ ç ”ç©¶å…´è¶£ç›¸ä¼¼åº¦åŒ¹é…")

        # æ£€æŸ¥æ˜¯å¦æœ‰æ•™æˆç»“æœ
        professor_results = None
        if hasattr(st.session_state, 'results') and st.session_state.results:
            # ç°åœ¨resultsä¸­åªåŒ…å«æ•™æˆé¡µé¢
            professor_results = st.session_state.results
            has_professors = len(professor_results) > 0
        else:
            st.info("â„¹ï¸ è¯·å…ˆåœ¨'ğŸ” ç½‘ç«™åˆ†æ'æ ‡ç­¾ä¸­åˆ†æä¸€ä¸ªç½‘ç«™ä»¥è·å–æ•™æˆç ”ç©¶å…´è¶£")
            professor_results = []
            has_professors = False

        # ç”¨æˆ·è¾“å…¥ç ”ç©¶å…´è¶£
        user_interests = st.text_area(
            "âœï¸ è¾“å…¥æ‚¨çš„ç ”ç©¶å…´è¶£ï¼ˆè¯·è¯¦ç»†æè¿°æ‚¨æ„Ÿå…´è¶£çš„ç ”ç©¶é¢†åŸŸã€æ–¹æ³•å’Œä¸»é¢˜ï¼‰", height=150, key="user_interests_input"
        )
        has_interests = bool(user_interests.strip())

        # å§‹ç»ˆæ˜¾ç¤ºæŒ‰é’®ï¼Œä½†æ ¹æ®æ¡ä»¶ç¦ç”¨
        col1, col2 = st.columns([3, 1])
        with col1:
            calc_button = st.button(
                "ğŸ” è®¡ç®—ç›¸ä¼¼åº¦", disabled=not has_professors or not has_interests, key="calc_similarity_button"
            )
            if not has_professors and not has_interests:
                st.caption("è¯·å…ˆåˆ†æç½‘ç«™å¹¶è¾“å…¥ç ”ç©¶å…´è¶£")
            elif not has_professors:
                st.caption("è¯·å…ˆåˆ†æç½‘ç«™ä»¥è·å–æ•™æˆæ•°æ®")
            elif not has_interests:
                st.caption("è¯·è¾“å…¥æ‚¨çš„ç ”ç©¶å…´è¶£")
        with col2:
            # æ£€æŸ¥æ˜¯å¦æœ‰ç›¸ä¼¼åº¦ç»“æœæ•°æ®
            has_similarity_results = (
                hasattr(st.session_state, 'similarity_results') and
                st.session_state.similarity_results is not None and
                len(st.session_state.similarity_results) > 0
            )
            
            sort_button = st.button(
                "ğŸ“Š æ’åºç»“æœ",
                disabled=not has_similarity_results,
                key="sort_results_button"
            )
            if not has_similarity_results:
                st.caption("è¯·å…ˆè®¡ç®—ç›¸ä¼¼åº¦")

        # å¤„ç†è®¡ç®—ç›¸ä¼¼åº¦æŒ‰é’®
        if calc_button and has_professors and has_interests:
            if not st.session_state.api_key:
                st.error("âŒ è¯·å…ˆåœ¨ä¾§è¾¹æ è®¾ç½®APIå¯†é’¥")
            else:
                st.subheader("ğŸ“Š ç›¸ä¼¼åº¦åˆ†æç»“æœ")

                # æ˜¾ç¤ºè¿›åº¦æŒ‡ç¤ºå™¨
                progress_container = st.empty()
                progress_bar = st.progress(0)
                progress_text = st.empty()

                # ä¸ºæ¯ä½æ•™æˆè®¡ç®—ç›¸ä¼¼åº¦
                results = []
                total_professors = len(professor_results)

                for i, professor in enumerate(professor_results):
                    # æ›´æ–°è¿›åº¦
                    progress = (i) / total_professors
                    progress_bar.progress(progress)
                    progress_text.text(
                        f"â³ æ­£åœ¨è®¡ç®—ç¬¬ {i+1}/{total_professors} ä½æ•™æˆçš„ç›¸ä¼¼åº¦..."
                    )
                    progress_container.info(f"ğŸ”„ åˆ†æä¸­: {professor.get('Professor Name', 'Unknown')}")

                    # è®¡ç®—ç›¸ä¼¼åº¦
                    similarity = calculate_advanced_similarity(
                        professor.get("Research Interests", ""),
                        professor.get("Keywords", []),
                        user_interests,
                        st.session_state.api_key,
                    )
                    # æå–ç›¸ä¼¼åº¦åˆ†æ•°
                    similarity_data = extract_structured_similarity_data(similarity)
                    
                    # éªŒè¯åˆ†æ•°ä¸€è‡´æ€§
                    if not validate_similarity_scores(similarity_data):
                        st.warning(f"âš ï¸ {professor.get('Professor Name', 'Unknown')} çš„ç›¸ä¼¼åº¦åˆ†æ•°å¯èƒ½ä¸å‡†ç¡®")

                    results.append(
                        {
                            "URL": professor.get("URL", ""),
                            "Professor Name": professor.get("Professor Name", ""),
                            "Title": professor.get("Title", ""),
                            "Department": professor.get("Department", ""),
                            "Research Interests": professor.get("Research Interests", ""),
                            "Keywords": professor.get("Keywords", []),
                            "Additional URLs": professor.get("Additional URLs", []),
                            "Confidence Score": professor.get("Confidence Score", 0),
                            "Similarity Analysis": similarity,
                            "Score": similarity_data['overall_score'],
                            "Dimension Scores": similarity_data.get('dimension_scores', {}),
                            "Matched Keywords": similarity_data.get('matched_keywords', []),
                            "Similarity Reasoning": similarity_data.get('reasoning', {}),
                            "Analysis Confidence": similarity_data.get('confidence', 0.0),
                            "Parsing Success": similarity_data.get('success', False)
                        }
                    )

                # å®Œæˆæ‰€æœ‰è®¡ç®—
                progress_bar.progress(1.0)
                progress_text.text("âœ… ç›¸ä¼¼åº¦è®¡ç®—å®Œæˆ!")
                time.sleep(0.5)  # çŸ­æš‚æ˜¾ç¤ºå®ŒæˆçŠ¶æ€
                progress_bar.empty()
                progress_text.empty()
                progress_container.empty()

                # ä¿å­˜ç»“æœåˆ°ä¼šè¯çŠ¶æ€
                st.session_state.similarity_results = pd.DataFrame(results)

                # æ˜¾ç¤ºç»“æœ
                display_advanced_similarity_results(st.session_state.similarity_results)

        # å¤„ç†æ’åºæŒ‰é’®
        elif (
            sort_button and has_similarity_results
        ):
            try:
                # ç¡®ä¿æ•°æ®æ˜¯DataFrameæ ¼å¼
                if isinstance(st.session_state.similarity_results, pd.DataFrame):
                    sorted_results = st.session_state.similarity_results.sort_values(
                        by="Score", ascending=False
                    )
                else:
                    # å¦‚æœä¸æ˜¯DataFrameï¼Œå…ˆè½¬æ¢
                    sorted_results = pd.DataFrame(st.session_state.similarity_results).sort_values(
                        by="Score", ascending=False
                    )

                st.subheader("ğŸ“Š æ’åºåçš„ç›¸ä¼¼åº¦åˆ†æç»“æœ")
                display_advanced_similarity_results(sorted_results)
            except Exception as e:
                st.error(f"æ’åºå¤±è´¥: {str(e)}")
                st.write("å°è¯•æ˜¾ç¤ºåŸå§‹ç»“æœ:")
                display_advanced_similarity_results(st.session_state.similarity_results)

        # å¦‚æœå·²æœ‰ç»“æœä½†æœªç‚¹å‡»ä»»ä½•æŒ‰é’®ï¼Œæ˜¾ç¤ºä¹‹å‰çš„ç»“æœ
        elif has_similarity_results:
            st.subheader("ğŸ“Š ä¸Šæ¬¡ç›¸ä¼¼åº¦åˆ†æç»“æœ")
            display_advanced_similarity_results(st.session_state.similarity_results)


if __name__ == "__main__":
    main()
